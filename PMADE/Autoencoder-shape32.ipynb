{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "from loader import get_train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Autoencoder16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder16, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=2, padding=0),\n",
    "#             nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=2, padding=0),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=2, padding=1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 16, kernel_size=2, padding=1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16, 16, 2, stride=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ConvTranspose2d(16, 16, 2, stride=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 3, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        latent_code = self.encoder(x)\n",
    "#         print(latent_code.shape)\n",
    "        reconstruction = self.decoder(latent_code)\n",
    "        return reconstruction, latent_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create MSE loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "autoencoder = Autoencoder16().cuda()\n",
    "\n",
    "# Use Adam optimizer\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader, val_loader = get_train_loader(\"../deepfashion/index.p\", batch_size=64, resize_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 67.844s\n",
      "  training loss (in-iteration): \t0.021641\n",
      "  validation loss: \t\t\t0.008521\n",
      "Epoch 2 of 100 took 64.264s\n",
      "  training loss (in-iteration): \t0.003836\n",
      "  validation loss: \t\t\t0.003772\n",
      "Epoch 3 of 100 took 64.740s\n",
      "  training loss (in-iteration): \t0.002701\n",
      "  validation loss: \t\t\t0.004170\n",
      "Epoch 4 of 100 took 62.875s\n",
      "  training loss (in-iteration): \t0.002374\n",
      "  validation loss: \t\t\t0.004026\n",
      "Epoch 5 of 100 took 63.957s\n",
      "  training loss (in-iteration): \t0.002186\n",
      "  validation loss: \t\t\t0.004379\n",
      "Epoch 6 of 100 took 63.730s\n",
      "  training loss (in-iteration): \t0.002058\n",
      "  validation loss: \t\t\t0.003739\n",
      "Epoch 7 of 100 took 63.874s\n",
      "  training loss (in-iteration): \t0.001964\n",
      "  validation loss: \t\t\t0.003051\n",
      "Epoch 8 of 100 took 62.566s\n",
      "  training loss (in-iteration): \t0.001891\n",
      "  validation loss: \t\t\t0.002908\n",
      "Epoch 9 of 100 took 64.254s\n",
      "  training loss (in-iteration): \t0.001828\n",
      "  validation loss: \t\t\t0.002771\n",
      "Epoch 10 of 100 took 62.701s\n",
      "  training loss (in-iteration): \t0.001775\n",
      "  validation loss: \t\t\t0.002958\n",
      "Epoch 11 of 100 took 66.012s\n",
      "  training loss (in-iteration): \t0.001726\n",
      "  validation loss: \t\t\t0.002217\n",
      "Epoch 12 of 100 took 84.048s\n",
      "  training loss (in-iteration): \t0.001679\n",
      "  validation loss: \t\t\t0.002260\n",
      "Epoch 13 of 100 took 79.451s\n",
      "  training loss (in-iteration): \t0.001637\n",
      "  validation loss: \t\t\t0.001941\n",
      "Epoch 14 of 100 took 65.786s\n",
      "  training loss (in-iteration): \t0.001597\n",
      "  validation loss: \t\t\t0.001987\n",
      "Epoch 15 of 100 took 63.900s\n",
      "  training loss (in-iteration): \t0.001563\n",
      "  validation loss: \t\t\t0.001865\n",
      "Epoch 16 of 100 took 62.749s\n",
      "  training loss (in-iteration): \t0.001532\n",
      "  validation loss: \t\t\t0.001789\n",
      "Epoch 17 of 100 took 64.474s\n",
      "  training loss (in-iteration): \t0.001502\n",
      "  validation loss: \t\t\t0.001782\n",
      "Epoch 18 of 100 took 63.716s\n",
      "  training loss (in-iteration): \t0.001475\n",
      "  validation loss: \t\t\t0.001802\n",
      "Epoch 19 of 100 took 64.427s\n",
      "  training loss (in-iteration): \t0.001450\n",
      "  validation loss: \t\t\t0.001777\n",
      "Epoch 20 of 100 took 63.186s\n",
      "  training loss (in-iteration): \t0.001430\n",
      "  validation loss: \t\t\t0.001796\n",
      "Epoch 21 of 100 took 63.449s\n",
      "  training loss (in-iteration): \t0.001409\n",
      "  validation loss: \t\t\t0.001536\n",
      "Epoch 22 of 100 took 63.119s\n",
      "  training loss (in-iteration): \t0.001389\n",
      "  validation loss: \t\t\t0.001485\n",
      "Epoch 23 of 100 took 62.251s\n",
      "  training loss (in-iteration): \t0.001371\n",
      "  validation loss: \t\t\t0.001444\n",
      "Epoch 24 of 100 took 62.857s\n",
      "  training loss (in-iteration): \t0.001351\n",
      "  validation loss: \t\t\t0.001389\n",
      "Epoch 25 of 100 took 62.286s\n",
      "  training loss (in-iteration): \t0.001331\n",
      "  validation loss: \t\t\t0.001413\n",
      "Epoch 26 of 100 took 63.182s\n",
      "  training loss (in-iteration): \t0.001307\n",
      "  validation loss: \t\t\t0.001412\n",
      "Epoch 27 of 100 took 62.600s\n",
      "  training loss (in-iteration): \t0.001283\n",
      "  validation loss: \t\t\t0.001327\n",
      "Epoch 28 of 100 took 63.009s\n",
      "  training loss (in-iteration): \t0.001262\n",
      "  validation loss: \t\t\t0.001284\n",
      "Epoch 29 of 100 took 63.205s\n",
      "  training loss (in-iteration): \t0.001243\n",
      "  validation loss: \t\t\t0.001264\n",
      "Epoch 30 of 100 took 71.393s\n",
      "  training loss (in-iteration): \t0.001226\n",
      "  validation loss: \t\t\t0.001237\n",
      "Epoch 31 of 100 took 71.553s\n",
      "  training loss (in-iteration): \t0.001211\n",
      "  validation loss: \t\t\t0.001225\n",
      "Epoch 32 of 100 took 72.214s\n",
      "  training loss (in-iteration): \t0.001197\n",
      "  validation loss: \t\t\t0.001239\n",
      "Epoch 33 of 100 took 62.512s\n",
      "  training loss (in-iteration): \t0.001185\n",
      "  validation loss: \t\t\t0.001200\n",
      "Epoch 34 of 100 took 62.967s\n",
      "  training loss (in-iteration): \t0.001173\n",
      "  validation loss: \t\t\t0.001198\n",
      "Epoch 35 of 100 took 63.615s\n",
      "  training loss (in-iteration): \t0.001163\n",
      "  validation loss: \t\t\t0.001194\n",
      "Epoch 36 of 100 took 62.510s\n",
      "  training loss (in-iteration): \t0.001153\n",
      "  validation loss: \t\t\t0.001187\n",
      "Epoch 37 of 100 took 62.339s\n",
      "  training loss (in-iteration): \t0.001144\n",
      "  validation loss: \t\t\t0.001172\n",
      "Epoch 38 of 100 took 63.572s\n",
      "  training loss (in-iteration): \t0.001136\n",
      "  validation loss: \t\t\t0.001172\n",
      "Epoch 39 of 100 took 61.707s\n",
      "  training loss (in-iteration): \t0.001129\n",
      "  validation loss: \t\t\t0.001159\n",
      "Epoch 40 of 100 took 62.028s\n",
      "  training loss (in-iteration): \t0.001121\n",
      "  validation loss: \t\t\t0.001157\n",
      "Epoch 41 of 100 took 62.470s\n",
      "  training loss (in-iteration): \t0.001115\n",
      "  validation loss: \t\t\t0.001141\n",
      "Epoch 42 of 100 took 62.476s\n",
      "  training loss (in-iteration): \t0.001108\n",
      "  validation loss: \t\t\t0.001149\n",
      "Epoch 43 of 100 took 63.218s\n",
      "  training loss (in-iteration): \t0.001102\n",
      "  validation loss: \t\t\t0.001141\n",
      "Epoch 44 of 100 took 61.579s\n",
      "  training loss (in-iteration): \t0.001096\n",
      "  validation loss: \t\t\t0.001140\n",
      "Epoch 45 of 100 took 63.418s\n",
      "  training loss (in-iteration): \t0.001090\n",
      "  validation loss: \t\t\t0.001137\n",
      "Epoch 46 of 100 took 67.632s\n",
      "  training loss (in-iteration): \t0.001085\n",
      "  validation loss: \t\t\t0.001117\n",
      "Epoch 47 of 100 took 73.350s\n",
      "  training loss (in-iteration): \t0.001079\n",
      "  validation loss: \t\t\t0.001114\n",
      "Epoch 48 of 100 took 68.618s\n",
      "  training loss (in-iteration): \t0.001074\n",
      "  validation loss: \t\t\t0.001106\n",
      "Epoch 49 of 100 took 72.293s\n",
      "  training loss (in-iteration): \t0.001069\n",
      "  validation loss: \t\t\t0.001103\n",
      "Epoch 50 of 100 took 61.481s\n",
      "  training loss (in-iteration): \t0.001064\n",
      "  validation loss: \t\t\t0.001103\n",
      "Epoch 51 of 100 took 62.979s\n",
      "  training loss (in-iteration): \t0.001060\n",
      "  validation loss: \t\t\t0.001102\n",
      "Epoch 52 of 100 took 63.442s\n",
      "  training loss (in-iteration): \t0.001055\n",
      "  validation loss: \t\t\t0.001096\n",
      "Epoch 53 of 100 took 62.482s\n",
      "  training loss (in-iteration): \t0.001050\n",
      "  validation loss: \t\t\t0.001084\n",
      "Epoch 54 of 100 took 61.986s\n",
      "  training loss (in-iteration): \t0.001046\n",
      "  validation loss: \t\t\t0.001085\n",
      "Epoch 55 of 100 took 62.623s\n",
      "  training loss (in-iteration): \t0.001041\n",
      "  validation loss: \t\t\t0.001081\n",
      "Epoch 56 of 100 took 61.367s\n",
      "  training loss (in-iteration): \t0.001037\n",
      "  validation loss: \t\t\t0.001087\n",
      "Epoch 57 of 100 took 63.265s\n",
      "  training loss (in-iteration): \t0.001033\n",
      "  validation loss: \t\t\t0.001089\n",
      "Epoch 58 of 100 took 63.692s\n",
      "  training loss (in-iteration): \t0.001029\n",
      "  validation loss: \t\t\t0.001109\n",
      "Epoch 59 of 100 took 61.948s\n",
      "  training loss (in-iteration): \t0.001025\n",
      "  validation loss: \t\t\t0.001094\n",
      "Epoch 60 of 100 took 62.230s\n",
      "  training loss (in-iteration): \t0.001021\n",
      "  validation loss: \t\t\t0.001094\n",
      "Epoch 61 of 100 took 62.870s\n",
      "  training loss (in-iteration): \t0.001018\n",
      "  validation loss: \t\t\t0.001081\n",
      "Epoch 62 of 100 took 60.340s\n",
      "  training loss (in-iteration): \t0.001014\n",
      "  validation loss: \t\t\t0.001118\n",
      "Epoch 63 of 100 took 93.176s\n",
      "  training loss (in-iteration): \t0.001011\n",
      "  validation loss: \t\t\t0.001102\n",
      "Epoch 64 of 100 took 121.679s\n",
      "  training loss (in-iteration): \t0.001007\n",
      "  validation loss: \t\t\t0.001114\n",
      "Epoch 65 of 100 took 75.558s\n",
      "  training loss (in-iteration): \t0.001004\n",
      "  validation loss: \t\t\t0.001085\n",
      "Epoch 66 of 100 took 63.685s\n",
      "  training loss (in-iteration): \t0.001001\n",
      "  validation loss: \t\t\t0.001139\n",
      "Epoch 67 of 100 took 63.172s\n",
      "  training loss (in-iteration): \t0.000998\n",
      "  validation loss: \t\t\t0.001076\n",
      "Epoch 68 of 100 took 61.051s\n",
      "  training loss (in-iteration): \t0.000994\n",
      "  validation loss: \t\t\t0.001080\n",
      "Epoch 69 of 100 took 61.402s\n",
      "  training loss (in-iteration): \t0.000991\n",
      "  validation loss: \t\t\t0.001086\n",
      "Epoch 70 of 100 took 62.611s\n",
      "  training loss (in-iteration): \t0.000988\n",
      "  validation loss: \t\t\t0.001074\n",
      "Epoch 71 of 100 took 63.702s\n",
      "  training loss (in-iteration): \t0.000985\n",
      "  validation loss: \t\t\t0.001075\n",
      "Epoch 72 of 100 took 66.436s\n",
      "  training loss (in-iteration): \t0.000982\n",
      "  validation loss: \t\t\t0.001055\n",
      "Epoch 73 of 100 took 62.099s\n",
      "  training loss (in-iteration): \t0.000980\n",
      "  validation loss: \t\t\t0.001077\n",
      "Epoch 74 of 100 took 61.674s\n",
      "  training loss (in-iteration): \t0.000977\n",
      "  validation loss: \t\t\t0.001059\n",
      "Epoch 75 of 100 took 62.250s\n",
      "  training loss (in-iteration): \t0.000974\n",
      "  validation loss: \t\t\t0.001049\n",
      "Epoch 76 of 100 took 62.038s\n",
      "  training loss (in-iteration): \t0.000971\n",
      "  validation loss: \t\t\t0.001036\n",
      "Epoch 77 of 100 took 62.921s\n",
      "  training loss (in-iteration): \t0.000969\n",
      "  validation loss: \t\t\t0.001028\n",
      "Epoch 78 of 100 took 64.059s\n",
      "  training loss (in-iteration): \t0.000966\n",
      "  validation loss: \t\t\t0.001018\n",
      "Epoch 79 of 100 took 61.851s\n",
      "  training loss (in-iteration): \t0.000963\n",
      "  validation loss: \t\t\t0.001023\n",
      "Epoch 80 of 100 took 63.897s\n",
      "  training loss (in-iteration): \t0.000961\n",
      "  validation loss: \t\t\t0.001029\n",
      "Epoch 81 of 100 took 63.053s\n",
      "  training loss (in-iteration): \t0.000958\n",
      "  validation loss: \t\t\t0.001026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82 of 100 took 64.890s\n",
      "  training loss (in-iteration): \t0.000955\n",
      "  validation loss: \t\t\t0.001022\n",
      "Epoch 83 of 100 took 62.874s\n",
      "  training loss (in-iteration): \t0.000953\n",
      "  validation loss: \t\t\t0.001026\n",
      "Epoch 84 of 100 took 64.626s\n",
      "  training loss (in-iteration): \t0.000951\n",
      "  validation loss: \t\t\t0.001019\n",
      "Epoch 85 of 100 took 62.404s\n",
      "  training loss (in-iteration): \t0.000948\n",
      "  validation loss: \t\t\t0.001024\n",
      "Epoch 86 of 100 took 62.510s\n",
      "  training loss (in-iteration): \t0.000946\n",
      "  validation loss: \t\t\t0.001005\n",
      "Epoch 87 of 100 took 63.567s\n",
      "  training loss (in-iteration): \t0.000943\n",
      "  validation loss: \t\t\t0.001006\n",
      "Epoch 88 of 100 took 62.239s\n",
      "  training loss (in-iteration): \t0.000941\n",
      "  validation loss: \t\t\t0.000997\n",
      "Epoch 89 of 100 took 62.327s\n",
      "  training loss (in-iteration): \t0.000939\n",
      "  validation loss: \t\t\t0.000986\n",
      "Epoch 90 of 100 took 62.647s\n",
      "  training loss (in-iteration): \t0.000936\n",
      "  validation loss: \t\t\t0.000985\n",
      "Epoch 91 of 100 took 62.128s\n",
      "  training loss (in-iteration): \t0.000934\n",
      "  validation loss: \t\t\t0.000985\n",
      "Epoch 92 of 100 took 64.134s\n",
      "  training loss (in-iteration): \t0.000932\n",
      "  validation loss: \t\t\t0.000968\n",
      "Epoch 93 of 100 took 61.509s\n",
      "  training loss (in-iteration): \t0.000930\n",
      "  validation loss: \t\t\t0.000975\n",
      "Epoch 94 of 100 took 65.254s\n",
      "  training loss (in-iteration): \t0.000927\n",
      "  validation loss: \t\t\t0.000983\n",
      "Epoch 95 of 100 took 65.626s\n",
      "  training loss (in-iteration): \t0.000925\n",
      "  validation loss: \t\t\t0.000956\n",
      "Epoch 96 of 100 took 62.213s\n",
      "  training loss (in-iteration): \t0.000923\n",
      "  validation loss: \t\t\t0.000958\n",
      "Epoch 97 of 100 took 61.150s\n",
      "  training loss (in-iteration): \t0.000921\n",
      "  validation loss: \t\t\t0.000945\n",
      "Epoch 98 of 100 took 60.741s\n",
      "  training loss (in-iteration): \t0.000918\n",
      "  validation loss: \t\t\t0.000945\n",
      "Epoch 99 of 100 took 85.788s\n",
      "  training loss (in-iteration): \t0.000916\n",
      "  validation loss: \t\t\t0.000948\n",
      "Epoch 100 of 100 took 87.590s\n",
      "  training loss (in-iteration): \t0.000914\n",
      "  validation loss: \t\t\t0.000946\n"
     ]
    }
   ],
   "source": [
    "# Train your autoencoder\n",
    "# Visualize progress in reconstruction and loss decay\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "reconstructed_pictures = []\n",
    "\n",
    "import time\n",
    "num_epochs = 100 # total amount of full passes over training data\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    start_time = time.time()\n",
    "    autoencoder.train(True) # enable dropout / batch_norm training behavior\n",
    "    i = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        i += 1\n",
    "        # train on batch\n",
    "        X_batch_0 = torch.FloatTensor(X_batch[:, 0])\n",
    "        X_batch_0 = Variable(X_batch_0).cuda()\n",
    "#         X_batch = Variable(X_batch)\n",
    "#         y_batch = Variable(y_batch)\n",
    "        output_img, _ = autoencoder(X_batch_0)\n",
    "        loss = criterion(output_img, X_batch_0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.append(loss.cpu().data.numpy()[0])\n",
    "#         train_loss.append(loss.data.numpy())\n",
    "        \n",
    "        X_batch_1 = torch.FloatTensor(X_batch[:, 1])\n",
    "        X_batch_1 = Variable(X_batch_1).cuda()\n",
    "#         X_batch = Variable(X_batch)\n",
    "#         y_batch = Variable(y_batch)\n",
    "        output_img, _ = autoencoder(X_batch_1)\n",
    "        loss = criterion(output_img, X_batch_1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.append(loss.cpu().data.numpy()[0])\n",
    "#         train_loss.append(loss.data.numpy())\n",
    "#         print(i, \":\", loss.data.cpu().numpy()[0], end=\", \")\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    autoencoder.train(False) # disable dropout / use averages for batch_norm\n",
    "    for X_batch, y_batch in val_loader:\n",
    "        X_batch_0 = Variable(X_batch[:, 0]).cuda()\n",
    "        output_img, _ = autoencoder(X_batch_0)\n",
    "        val_loss.append(criterion(output_img, X_batch_0).cpu().data.numpy()[0])\n",
    "        X_batch_1 = Variable(X_batch[:, 1]).cuda()\n",
    "        output_img, _ = autoencoder(X_batch_1)\n",
    "        val_loss.append(criterion(output_img, X_batch_1).cpu().data.numpy()[0])\n",
    "#         val_loss.append(criterion(output_img, X_batch).data.numpy())\n",
    "#     if epoch % 16 == 0:\n",
    "#         X_batch = Variable(torch.FloatTensor(np.array([X_val[247]]))).cuda()\n",
    "#         output_img, _ = autoencoder(X_batch)\n",
    "#         # reconstructed_pictures.append(output_img.cpu().data.numpy()[0])\n",
    "#         reconstructed_pictures.append(output_img.data.numpy())\n",
    "    # Then we print the results for this epoch:\n",
    "    print \n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(train_loss[-2 * len(train_loader):])))\n",
    "    print(\"  validation loss: \\t\\t\\t{:.6f}\".format(\n",
    "        np.mean(val_loss[-2 * len(val_loader):])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_gallery(images, h, w, n_row=3, n_col=6):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    scale_const = 1.2\n",
    "    plt.figure(figsize=(3 / scale_const * n_col, 3.4 / scale_const * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].numpy().transpose(1,2,0), cmap=plt.cm.gray, vmin=-1, vmax=1, interpolation='nearest')\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder16(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (4): ReLU()\n",
       "    (5): Conv2d(32, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
       "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(32, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
       "    (9): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "    (10): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (11): ReLU()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): ConvTranspose2d(16, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ConvTranspose2d(16, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (6): ReLU()\n",
       "    (7): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAACxCAYAAADwMnaUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nG09QdcuMsmOSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f912780d7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_gallery(X_batch[:3, 1], 16, 16, n_row=1, n_col=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAACxCAYAAADwMnaUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9127d18ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_gallery(X_batch[:3, 0], 16, 16, n_row=1, n_col=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAACxCAYAAADwMnaUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJztncmvJUd63SOHO7+pJrLYJKvYFHsg\n2+6WjIZESF4IWtoGDHjtnddee6P/QCt7ZcB7e2XYawEGZAPdDXlgtyxZdnebTbI51lxvvENOXtAW\nkHF+hRfv1qt6Kev8dvdDZGZk5hdx42aee76s67pgjDHGGDME8qvugDHGGGPM/8MLE2OMMcYMBi9M\njDHGGDMYvDAxxhhjzGDwwsQYY4wxg8ELE2OMMcYMBi9MjDHGGDMYvDAxxhhjzGDwwsQYY4wxg8EL\nE2OMMcYMhvIijW/evNndvXv3RfUlNE2jQbDMRxN9CCab7Wf9j7Raq6q1xB4/uC+x07Mz7Qd0pMgy\nie3uLCS2f+Nm7/NoMoXe6b4Iuh5pW4YQoL85xLY+QNS5Tz75JDx8+DC5e5fJjRs3ujt37vRiGZxr\n0vWEm9+0rW6YmucEdgQuXRRq6lqaHD15LLF6vZLYqCwkVkIsL9Jio8k8+qx5TqfJJTXgWrZp15fu\nc1mW57Z5nrH105/+9GHXdbcSm18qX8/pd6Lo5Q275yl5kjrP8zTUb1htdP4+PXoqsc1a2+WZfiNM\n53OJzXf3ddvi/K/YNvEaJU+liZecrluen/+sgvM/7Qv4gw8+SMr1Cy1M7t69G378kx/1YkWuk0zK\nhWlhEXJ8fKLtWm1Xw4RKx+w6vYAt3N74FCZwc+599qHE/vW/+hcS+88f/Exi9AWwGE8k9ge/977E\n/sE//ie9z69+8zvSJmR6DyjZu1zPPWvT0r2AL53JZNz7nNP+UxYvIYQuus/vv/+7Sdu9CO7cuRP+\n5D/+SS9Gk1NHuRRdgw7y/PTkVGKU501HCxg9ZtNou6wcad+K/raHDx9Kmz/+t/9GYvd/9b8k9trN\nXYndvHlNYos9je3s7kns9re/3/v86tvvSZsazrOqKom1MCM28IXTwGKlKHVKvH6zP4+WI7229AWc\n0XiAe7q7u/eJBF8Sd+/eCT/6cX9OL+HLFOfX+DrD9dzA3EfQ9aMYrenHI+hv28+L+5/9Str86R//\ne4l9+uH/ltgEFsnv/dYPJfb93//7Ets9uCGxOI83lY59mjdzWnAnXje6fzRfzxf9BVcB34WpC5MO\nbtZ4PEnKdb/KMcYYY8xg8MLEGGOMMYPhQq9yQgghl9cGaY914lYp7wVDCKFt9FFgC4+IWnjMi6+9\n4Lht9DqjgeXaJz//C4l9/it9vbM70gO88Ya+UtuB9+y3dvS4XX0UBeCxHzx6zeBywJPWkMFFyuH1\nHL7hjDdNfZ0MNz+DVyVXRZZloYiuaQbnT6/L4secrIAg3Qnoq+iRKdx/0voU8Jg2RLG6Uu3Ik3uf\nS+yd129K7Ac//L7EplN95N022o+j+6rNuv+rv+x9Pnj1dWlTznSA0LWk12Jtq698ilxfydBrV9GD\n0XNxGlzwuy9LeIf/cslgvNPkfL6Khuf9ND0OA6/aIK/piq7Xy97nJ5/8ubQ5u/exxG4udG9tra8B\nv/ifH0js7vd+U2Lz/eu6v+i06GqX8P0Qv+4O4RkaHojRq+iUOZ11PmlCn+eZ04c2SowxxhjzNxgv\nTIwxxhgzGLwwMcYYY8xg8MLEGGOMMYPhwuJX5XwvB2pFXgMkWmPxDQldyQdCm2UQq+pN/3OjQrkv\nP9X/wefQ7huLscTevqVeDjPwQhg1S4kdfv6L3ufrr/+GtJns6H/l85z+2w9+F6hCJvGUtkq0KEk8\nZha12W7fl0EXVGAdi2FDeMY1Ebc+8tIhs68EcXHga0d5XsJPjvj+L4+faJuVGgTOp2ocNR6pGeBo\npN48AXTUeaG5/+DLT3uf3zzTvu3vqPh1Op9JrNpsJLYGP5kWBKsZCb/j+QamHxJ50jx4Eenny+P8\nwcZCxi5qk/hHCNgXXir63Uz/XoBQvep7BR1+9gtpM+80T157MzabC+H0SMfEl199JbGz+x9r18CP\nJ+5uDl9KHXzHobcJzQfPkNzHkGdPfIzzZ+q/2jDpmKn4iYkxxhhjBoMXJsYYY4wZDF6YGGOMMWYw\neGFijDHGmMFwCeLXRHvVOISGdeCICaI1LNxFwlkobldXKnhqosqTRdB9HVxXB79b17QY2dNHjyR2\ncqpF2/KFVqdcrbS/jz7qu8u+/u6htJntvSKxjsRq5I4LpBfjS3DERPfS8ytRPkdR0uenC9CftGJZ\nYuxJJqEguiQ3YxbA6f4aGG9U8K6MmtUgdC0L3deXX2mxv9GfqRPya6+/KrHd6yrMbsCWeLWO+kIC\nd3LfhfmBxJUZzAV5kebWKrcZ+qaO2NzuSlXdzyI+wcQ+Jkzpz5DVps4T2oyq35IjahvN88uHOi8H\ncHQtR7r/cgwOwfAd9ODTjyV297egWvG475BMguuioPOEOUIi4RkurNCMNpUD0L0iUXJa4cBU/MTE\nGGOMMYPBCxNjjDHGDAYvTIwxxhgzGLbQmMTvI+F9Lm0WBUmzgOZR8D6+Jt3JRt/ltdC3BjQmm1Xf\n2GwBpk1vfluNco6+/LXE/vy/adXJkOllXi61vxN4v1/FOgCoksrXO62yKRlDsZnS+e+d+R3z+WZ7\n//eg0cerfBffiTEUGUWx7kQSXdrge3LQ9UCahw7GA77LJS1KlDtZo9WFD/Y091en2m4JGpbR3oHE\n5tdUm9VlemKTr/pjKQcXM6qETfeA8rcAQ8MadAK0bXyE1Iq5yeZUVwxqviJoOIopYGKuU1VuoqD5\nCu4P6bPief70THO4K9QMc7SrObyY6P5nDx5r3yg/W61WHZNDJWF0AqV5GdqhxoqqWqMh3vmkTs3P\nM4f7iYkxxhhjBoMXJsYYY4wZDF6YGGOMMWYweGFijDHGmMFwYfFrLJTKSABIwtYEVQ0ZyDS1ioc2\nUD2UDNZaqP5Lxjih7m/bQZsb19XE7G/94Dcllp891ViuJ396fCyxSa7nMJ70145tp9ejhuvWQrVl\nMvYqqMqzRHgFK7kAbUhOJQJRaJciyHupwLVLchAig7VEM6ImtRo0bEwmhGVUTTVvtZr1/lynhM8+\n1Uq/KxhGb3/rHYlNZlqFmMbl7rzfrt1onpPIkSoV061KFf+l6PVQRI6C8b8exGONBcVp1a/TDphm\nptbCvAn/D8COxKLTrgKjMxSIg4kZVGovp2qQ2YDJXrJYXRrRNUoz7Mu47Dkc4/xYqujf1YWNMcYY\n8/8tXpgYY4wxZjB4YWKMMcaYweCFiTHGGGMGwxbOr32RS3pFyS1JrLBKDoNknZlDZdMiFlk1KpTq\nQEy6v78jsXe/913dFvp78vCexKpTFc4W0flvTrS6cAdiwgxUYom6ycCC1bTKxCkkCQyv2CIzvi5d\n6hI+xUUzcVfsjgzt4F43kBN15PSa1SfSZpzpdidHmnPkXLw61gquWf6u9q0DF9Zs0vv85KGOhWam\nItzFvlYvbuEKVxWIwVM1fPF2IJAscLwlVta+YrSfaeLUpPLClO3wTwiav/GPFTAPNbXmbLXu5/Zm\npbleQHXh9uiBtpvsSmye6zEn9LcB+N4QcSqXUdYYkei8jrFLtCa+7OnaT0yMMcYYMxi8MDHGGGPM\nYPDCxBhjjDGDwQsTY4wxxgyGC4tfY6FUohGfNCQRWAti1RpiDZRcR1ErxNq1Oq6OIwFRu1Rn2Wp1\nJLFRpu0ObtyUWFaq2G8xm0hs80TLcK8j0d7Z4/vSZn+tDp7lTI+ZY7n2RJfABJFVqmA1RW97tRrB\nLEkQiCLhhAjleduAoy+4t6YacDag7Nwc98Wjx09UYNrB2KJTPwUx6YMTHQ93Gp1iHp7q/p7m/XLz\nmwdn0mYvV2HinZE6y45KPWbdpLmZ5rm6d6aUb8+SJYfDE79KUl2ikpHPFoTCFAORbEPfEWsQYp/2\n58SunEkb+lXedjoH17W2nCyu6f6me9oP+C5ZzPvnVUC+IugsS39KSHONxbn/3D09O6rHTGqG+ImJ\nMcYYYwaDFybGGGOMGQxemBhjjDFmMHhhYowxxpjBsIXza6xo2a50ODWicvAk4iNBbJmpeLBbqtD1\n7P4nEiuqviPmCJZrk7EKj4qFCu+KqYqsislU9zcFgd6eOsmGSIzXjVQ0W5+oi2wOpa+LMewfBFUs\nTqNmsQswODqmKmJjUfWVWr92Ic7zDN1wYdMUV1ssS57o1ItyQvp9oe1Oj/vjYXW2kjaHpyq4PW50\n/8tC8/BJp/n1tFYx4fFMBeI3v9uPVSBW3RSaq2cr7e98AteDXFhhbmkycoiNxIrgyomOrhC7akdj\npItzXdl2Tk8GNl2t9d5++bnOdY/v6x8CHn3eF0rXi9elzWKqfxBYjV+T2Jl+tYTDjebi059r3375\n9D9J7Dt/54e9z+99+660GU903NDIb0HUSnMuirphfyk8j3N1Kn5iYowxxpjB4IWJMcYYYwaDFybG\nGGOMGQxemBhjjDFmMFxY/Coir0RzQ9HBYEnrNAFZDWWu81wVSqsTLZP+5IuPJLZo+y6BxUyFR6NX\nVBSVNSqeygq9pBmUoG8hNpqocDaLxK5NDSLfMz3Pbkz70lgHjq4kCuxyiEXtUKxJ9xRFUV306Yod\nMi/NehZE3iS6RDdYcmFNKyNPx90s+3m+PFJx+KOHj3U7cJe8+dYdiY1fe0tiSygZH8Y6Rvbm/Tyv\n1uqYGcAdt+vADbMDl2ISBMKlzEHYqg7HJFROdFC+9ALxz088RmnMJgkZaajD+eYFiZN14w+/UDHp\nH/3zf6mHrTQ/Xz3o591EzbHDQvXbYXSoDt/H4Cz7EfTt0y+/ktjp6r9IbPff/Yfe5z/8Z/9U2vzd\n938gMboF5OYdYH7B/zjQtBHnAm6WqPp/jjnUT0yMMcYYMxi8MDHGGGPMYPDCxBhjjDGDwQsTY4wx\nxgyGLZxfU6wtKRaX1qbSzCDaAaENaafI6bQBkezy9ERio67vgLmsVSm1f/OGxMZjLXPdkkMo9LdZ\nq+tm1qmYNo/2NwHn13xzqAdYqitt2FHHza4DsS70l0Ws0krbAEkmkkl7enHEZ5Iq4xKNNzZKFYql\nuc2C1i20IKY9fPSw9/mjv/yltPn1x1/ovnJ1Ln7ttjppvnrrFYnVJE7NVVy+buOk0+2owPvjMz3P\nutPYDOycy1z3OCpBDB7d1RbEwCToRIn3pYmqL5EEYWuShj3RMpacScnh+7/+919I7E9//GOJFYXO\ndfv7B/1urHRO71rNk8lc3YsL+G45PjuV2JPDpxLLcp2vT04+7X3+0U9+Jm1+97e/D/uSELpId+Ca\nnG5SnTCDpf6f4TkmcT8xMcYYY8xg8MLEGGOMMYPBCxNjjDHGDAYvTIwxxhgzGC4sfo0dAFHfgvqu\nKAhqKnLEbEGgFEB8RsKgvCS7OxDLRfs7fKyOmDu7DyU2mqmrJYlf2xqcPpcqfm1qPW4erR3LyVzb\nTPQ8810VJ2Y5ONVCOWy6qxk5YqbkArBlerxEspBJQiU6rqZoxyB/28TxQCozctfsOo0dHvaF3589\nUNH0vRN1XB3duCax3QMVumYjFckertStdbSjOdxGCvEW+l+MNH/XtY7nDcQWUxB5SyTN+fUChd8h\nNjzxq7q6pvU7FgXjnxcShZLkJvr0sTpao3BWdxcOT/turV2leVjDHxDype5tNNLc6UBc3oFIvAQH\n7nHTH4eHD9QxllyfS5iryTE6Q+de+KpPcCtONS9OmdMvgp+YGGOMMWYweGFijDHGmMHghYkxxhhj\nBsMWBmvxiyOqtAmbJbyWbcAYqYZ3aJuNVnuk928jeCedl3rKTdPf39lGj3l6ogY9czJJoxdrhb4v\nrNZq/kZVfYuyb0Z19OUDaTNd6HnuvAV6ksS35VxJNMVYL7HqasK2ydu9ELqQZLGW5okmpF7fFmI1\nVN2dgilUDTqLLx7132038+vSpjzQ3yrX3/yOxG7f/bbE2kLfp680zUNen181mSamAi7bqlXtQAFz\nAekfWphvmgb0OtFNzakid2Jl9CtN62cR9ZP0eikdx6rLUJUcFSwwz4czNSybQWXqk5WOiTr0jc2y\nTI3OukLzZEPakUINATNQtuRQWZ4mhLo+iwL63VKBOWg51v7SbUFtGpHQLNWYjbUo2ye7n5gYY4wx\nZjB4YWKMMcaYweCFiTHGGGMGgxcmxhhjjBkMFxa/ip4rVdsYG2hBm6ZWIVsDRjNVpcKgOtdtSzDG\nycB0rav77cqFignzhVYSzkYq9stAUJeNVDzVnKiZWpuBQc+0f9zHjz6WNvNTFbq+Alqy2BApBK5M\nCwUrUcinIWqTWoqyiz5dsRFVLAikJlvumszUGqpYi2Zq5xvdhRDCeqP7O400dm/cfU/a3LwL4te3\nvyuxW994Q2LHtW7bgGEZifOapj9+46raIYQAOspQUAxEjQX0I2/1upUJBmso8kSzqiEqXZV4jHKV\n96QdQez84329f429/923Jfbp31bR9Y//4kOJPYrm16yE+RvyMANjP6p0jfM8bLlT6jGuRaZrv/O2\njqWC5lKYI5KKvocLVLVOaZf4P4DncVjzExNjjDHGDAYvTIwxxhgzGLwwMcYYY8xg8MLEGGOMMYNh\nC+fX80l3E+2zgQqQVaWufhWIZOtS91+BQ2zVgPCu64ubsrGKUMcgfg05uMjCOeQgqAJjzjBdqJi2\nGPUdC1dn6hI4LUiIBe6aKE4lQRWI+xJEVs9Tc3WbNi+SBN/XrUHxJIjpWhC7NbWOh7ZScfUa8iS+\n1Tt7mtOv3NSq1NffVsHhZKpj5MkjFXRnARyIQYAeu6nmOVVXhSqvXCNYIiSi5yrPurdYD0gVz8n1\nkwSdV1s1+xkkiXTPd/hGI2TI645Ex4Xes++8o6LQf/j+9yR28vCxxH7y4b3e5xrcVWleJnfsMuic\n3nW6v8lIz+G1A62k/Y9+5/u9z3/w935f9zXWcQNfXWHbSvAhhGdManE7+h6BrS55wvYTE2OMMcYM\nBi9MjDHGGDMYvDAxxhhjzGDwwsQYY4wxg+HC4tdYRENiIdTUJKi+Gig5vYHyzzWIXzfgOhk2KhTM\nqCR6dE4bKMuez3clttZuhNBpee3x5BrsT8+13DmQWFb2RYbFTPvRZSDOSlHxfX0EiZAzaY6Cqvhz\nWkFszo/o45WKBLOQSe13cvakTeNg2n0gwTFeA8jfCvL16eGZxB4e9gV7N67dkjaT67f1mDN1Ql7V\ncA5w/8GENZCgrhDxq+YbCddp/hmhuDYtoXKwPdbTeh6Z9/DUrzpuU/udcB2gSQuCe3KbnoED91t3\nviGx737jpsQ++OiL/jHXOh7yXAWm5BDcwvjKmpVuCwLet67fkNjv/XbfcXnvxnXdP1yQeIyEEEKb\ngTgVJw6y+IbvzHhOT5z3Uub0i+AnJsYYY4wZDF6YGGOMMWYweGFijDHGmMHghYkxxhhjBsNLc35N\n0UnV4BJYgUXqGhxdJyQWApFVAQK9uuofo16Ts6O6smaThe5/pI6YYaoirvJA23VjFc4WkQvtzi11\nQ9x88Us9ZqXXKKBjpQrASDebXuo62n+yTjASVV9pyfguJHm/Ypn3lKtCgjKNgVlpWIPiOh9rHi5b\nFYAetn2H2Hb/VWlT7KlYr4ZpYl1p5yoQxE7IwRUcV4soNys4+SWIEJu15nlT6diqQCSYw3yzAcF8\nfFY0h2BZeYhdaVo/i6ifoLHcejyqiPwZbtMwN9G24x0V/5PDrsh5QejZgSC66cjlVUK4vxG4j+/q\nlB7KrP+Hjgz6kfodyrcl7V6hsDUiVfLM3dg+2f3ExBhjjDGDwQsTY4wxxgwGL0yMMcYYMxi8MDHG\nGGPMYLiw+DUWeZFwh1wsxQ+T3C/B+ZVKk1MJ8xZcIUcgvjk9UQfA6slR7/MKHF2rRi9VmWtJ61Cq\n0LXJtCx9NlJxYjZW0V4+7aunpuDWefrpzyVWL7UEfVmpsK+FvnXkMIjiPg1JE1SOne8s2V21Q2ac\n56mnn2D82kKQhK453IeigDwsNVZMQAAa5Ws7U6fhqtB8aGBsjeG8ipH2YwPi9REIYpfrfqwDF82T\nJYje8VqCqzI40OaF5iFdcxVS0vyWLAkcHPEYxTELpAhiqQ05BMMtCy2o8OuNOq4+Pj6SmJ4D5Am4\nikM3QguDfwTzN06R4FK+fnS/v//1UtqUC3WD7chFmwTW1BGYX0JO39161PPbPIvt53A/MTHGGGPM\nYPDCxBhjjDGDwQsTY4wxxgwGL0yMMcYYMxgu7vwaCZewMjs61MXOntqGRK1No+IhErc14Aq5aVUo\ntQShVLbut6sqEMCBEGs8VTfYBsS6VHK9odLRjYqxQuiLEcspOMbCvqqTJ7qn9ak2LPUc8pIEgKSe\nkp5A7HwhNEVTXAlfJLHQjK4xnm6C3gt/DbSguIad1ZXmdNOqG2YHgtXVsr/tZq35tgHl9whufbFQ\nx2AU3ZFQncZqPCHAOKo2IGoFESI5F1eVbgumxyzKj0ItnFNRgJidFNNDJGFOTxmNNGbJ5ZXI4Uuj\nhVx/eu8riX31ROf0TfQdUYMIm/6kEXJQSYMguoL8BMPwcP9Eha1Hx/0/YJCgN/WLOdmRN7Fdgnaf\nnXbRDTnpkIifmBhjjDFmMHhhYowxxpjB4IWJMcYYYwaDFybGGGOMGQwXFr+KCyIJnmhDcRekfac5\n25HAdAUOe+R0Wm203TgSszXQpl6pQKkjZ8JKtyVhEMXaWvvbVJFCD5w/a7jiyycPJDZdqestGNU+\nQ79J5xB/TjtPYlv3yRdDBv0mRTdtGgUTHXPRwRF2X4xUsbmpddszUOLF46sgwWGn261gX02l9dxJ\n51h3+tunpt9D0SkU0CYDYeLuXMXgE7hGOQgdc3CgJRHm9tXbaW4cniA2j/+YsKXwPPXc4uN9fVCN\nVRudD+89eCSxjx8fSuxo1RedZp3mzgiFrtQ3ULVCszWIoh+fqvj1+PCk97le63mmQm6wqLlOvObx\n7nKc99KErs+T6X5iYowxxpjB4IWJMcYYYwaDFybGGGOMGQwXN1hLAN9QJry2bMDEqwY9yQb0JJOg\nsYLe28N7xSoy3+nAAOrs8LHErhfvSGw0guqsVG0Zqqe2S9WAtJFOpqBqj2AedPzVlxLbfUeNiEZw\njVq45gH6K/2AWOrbajXgu0qDtS5oOU5yxNvSYY12Bc1qMkkDk7FRqbEaxtJ0p2+m10At1Qwqjgao\n+t1Was5G+qqCdBZQ1bWKDgtNwhj2tTvT8ZajiEdDDWjENnBecSuqLpxa5fVq85qJ+57DUN+236lb\ntWCauTnT+fDxsWr9jiv4joj0HjnM+3RKXaMVrLsOTD6pDHHQYyzhu2pV9Y/RNXpONLeAzxub+CUY\nnH4d03bSDYilSOue1S4VPzExxhhjzGDwwsQYY4wxg8ELE2OMMcYMBi9MjDHGGDMYthC/xnIYqsZ5\nfkVZEou1IBSiWAfKow7MktDEDUScedHv3Xii/W9AoERVeMvpXGJU7ZSqpza1GkMVZf8WTRZaDXi2\np9VlT4/UdGh9quLXGdw/Er92IIqM7yEW4P1rIgAUom6nFhdOkXyxGRGZncE1BwXcGsbIZqMizr39\na73PkwlUls6gWm+hfaupbxIJoRxrTpcg2GsjoS/ofkMOY5eE5aCFZEEsibzJ2C3+DPcAzSG3FBy+\nbLY1N9x2HKNknG4PXOcV5cVY87iY9MfEdHFd2ow6MrSE6t0wJsbwJ4ednYXEbt7QvpWjvjkhS+gT\n7cloLqH5G1Ndzyvl1r8M70s/MTHGGGPMYPDCxBhjjDGDwQsTY4wxxgwGL0yMMcYYMxi2EL/2BU+k\ngwFjxCRnT3JibMHVlBwVO6js2DYqACwLqDwaCfTOVloRsgZHSHIrzKE6K4mR0NkPRIZZ0RcoTfdV\nxHXjm+pAe/TLP5NYs1YnRXJ5hJqboUP1VAx6AibGhlV1NRajkjg1zdoQ1Wm6K8gluubkjtyAGJwc\nTBfzvhBvRu7Da3BlhbG6BnfkFVTlHo21CnFRUob1tz1ZgRAenGX3ocrxFAS3rEDfLudIHF7AQMIq\nvVdaNfsZJIlYz2+TOqpp/i4gF7sGxKmN5sBvfOtbur9v9HNsMTuQNmWr+zo7O5FYU6rQNQPX5Ddf\nvy2x9w70XHdf7e+vhfGbPEOSwJq+gBNzUf6kgntK+0PD86S6n5gYY4wxZjB4YWKMMcaYweCFiTHG\nGGMGgxcmxhhjjBkMW4hf+6DjIYpezlfCFCCgKUAQSkKzGkSyHQlnSxXGdZED3nRH3VsDOP1Va3UJ\nLKbq/kfutSSeItVwHgkFRzN1Ety9/YbEnoL4tVqqsIvtcUnoiTW3z98MNXPnu8imuk++GLIkV0su\n/x2JZuE0SMBXQY6Q+2lRUGl1FWMuQYg6mU77gVKH/woOWsO1WMExK+hvDa6ZNeRSGzVbdSrCfXp4\nKrGDhY7nPXBuLkhwDGMwz0DoGM03z2PeiiLqKyaX+3t5jq48v1BDcBeG2KZUsfPils5/B9f6c3iR\n6Xb5ai2xZn5DYiv4Y8UUvjmnBzcltntDG26ap/1joivr+fNtCOEZtrGJQn38l0rUBDajfrCb9fb4\niYkxxhhjBoMXJsYYY4wZDF6YGGOMMWYweGFijDHGmMHw3OLXrQ0xAdLs4CFJiAnqm8lkqrH5jsTO\njg57n6/dUAEUGUceP34ksfFiT2I1OAzmGbhHFno7urYvAmxqPfciFjWGELKZinA3S3V+bWt1V2zh\nPuSj89ewqZordCuUJlcpEuxCLN3i3pzv6tpB4tQgdK3AqXVdqThzBZrpdaUC02qt7sWTWV/YuSFh\nbgcOnDC2qpXmDbYDJ9mCpp2sv+1qrftfgyPzybHu66zQ61tkeuFGFCsThH3kUpqo8URTzismvm+p\nussUaBzTrsj9tAKn3zDfl9DZU912XfT/JNBqGobxSOfIZa1/aKiC9qMcqeh6GSu4QwhPYcA2kXI2\nH6kwF12DYb7JIaG6RGfWlFvKzq8Qu8R1QAh+YmKMMcaYAeGFiTHGGGMGgxcmxhhjjBkMXpgYY4wx\nZjBsIX6N5DCg+upShDaw3QhvR6A4AAAI/ElEQVSEPCMQmlGsqyCW6+m1GZxyJDrde+UVaZJPdyW2\nrEGw2GisJQEkyIro/JtN350wB7HqGErLz2+9JrGzlQq7mkpj3UhFXC04Z2ouJCqghlj6/VwSZWDi\nhqttKB/I/bFtQSBNwm9Q9lUrdfktZ9d6nzsYRyR8HoNbcgGCwEmpeTMq9BzKXGN51Jcc3HFnhV63\nMYjIq42OkXwMYwtyukMbzmhfyUq/5OL1V8q2rsuS24mnRs0aynVw2752oOLX8v6hxJrIhrgpdI7s\nOs3XTatusFmu/RhP1YG7gcFTgUNyKKL95Tq+CDR5Jf09fSfD9UU373hKT+zIZWe1n5gYY4wxZjB4\nYWKMMcaYweCFiTHGGGMGgxcmxhhjjBkMW4hfL6ccPJZOBlFgB46AJORpSFAIbppUNr6Y9IVRJbi3\nBnAJbEBcW4F4LqNjQsn5cjaXWL3ui1Ohinwo5yrMnd1+S2JPvvhEYtVa3WBHE3XHZSfW7bz9aF+p\ngruXBXgnJrVSbThK1nTvHbiVhgTBcQghwD0My2MJTRYHvc/zEZSaB7FeDqLTrNJYSyJc6G8R1Km4\nHPcFgNlaRdnXVIMYDuYqHJxNQPQO7stNq3MLCY7jdKU2OdmlAgNL8xDCJbosw25QMg4XgQTFJTjz\n7pZ67d+6psLWKprT74E7bE1KbxCEjsaaYxMQjs9gd/QdlJf9eb4oSZgL34XJ8+35ovzUTbPEfL1s\no24/MTHGGGPMYPDCxBhjjDGDwQsTY4wxxgwGL0yMMcYYMxi2EL/GJJQJ/zp67p5IQEbCrBwEShWs\nsTZQXp6cX4tpX+w5mqrQdQ1Cqc1GhVirjYqs5gsVN5GKNY8dAUMIWdYXT1HZ9CzXfWUTOIdM+0HG\nhFNw+uzoGKJnTlNAoVthlB9UuvulkWUgxksVj53v/Ernn4GQuigg90k4WIFb5UrFr+Om74g6JhEn\nuPfGrqwhhNBVsG0AgeFGx+ApKOq6Vf8c6uWptJntal5OwaV4MtFxtAaxbgMl6Uu45pIJqYaudJ4D\ndH7VHN1yHLMNqYYSrwG58E6Diknf1P8MhFXXv7dHjzSfjlaa1wX0twyaYzP4SV+CK3cIGtus+2N9\nA07FmE40l0C7ZBLm69T/PFy2qNtPTIwxxhgzGLwwMcYYY8xg8MLEGGOMMYPh4hoTLROsTbZ0W+nI\npAj0JKRFIYOe8VgrQK6hyuTJcd/MaXWk7+cLMF2jqr45aDFy0LVMoHImrRJj7UG90nely1rflZ89\neaqxY912s4b3m6llLLeFdDKRtiHb0rztxZFmlBZfpxauG5kB4ut5MvuCirhTqLB7faw7nHXL3udy\ns5Q241xzekTVgGH/AQzLVqAT2HSa6UfH/b50K60YW87V+K8AXUsJqbOCSuBklFaBLk3OFOYkMoek\n9Lg0M7MXCHfxfGFNqi6CoHsxggq+86ma871ydqT72+nPw/VtFaJ8dqzHPF3p/Z9PVdv02q7O6QsY\nmzPI/8msvy1VuU69bhkmmYbo+zFpRoftktPjOeZwPzExxhhjzGDwwsQYY4wxg8ELE2OMMcYMBi9M\njDHGGDMYLix+FZFXsu9UvyFVT8xhZ1h1EsSvFQmIYH/LlYo9799/1Ps8AqHc7v517ceBxgrobzVV\ncdbOjlYEXoLRTrXuiwIPv/i1tJlAFdr7X30pMZI7bZZambYBMW0oVdQb30ISYqVLZodjsJaF7WVb\nIlqDPK8qMP6j6tgQyxvdNms1byZg7LRb9PN6Z6r7X+yqSLAA8V99qmOkBIPAFZxrgP1N6pPe55OH\nT6TNHqREWauANzS6/xzMzlCYTNXBZV+aHS2ZqW1pNPmy6aK5swOTPdJiqsEatMEDQgz+NLD/2l2J\n3V7r/V6e/Exir8QK6NsqnN7Z0++Rx8c6Hy7GKrh9c0dzbAFfp2/cfl1i7777zf52BwfSBiGzOhRd\naw5jIWUQccfHoBxu6XEGCr2hXSJ+YmKMMcaYweCFiTHGGGMGgxcmxhhjjBkMXpgYY4wxZjA8d3Vh\nrJSaIHpJLEQZGhDPZSD4KUj8ullJbJGDU+2qL6h6+Im6pj7NP5XYzq0bEgtQFXXv1k2JHcNFunfv\nnu6v6osMzx5pm9ev70ssh37kIxVszWcqdkwVLcUC5i5RMprU6go1gh0cnoTUJIKMmzXgaLkCt92m\nUjFp3qkIOQN31QBi7RGUoZ5EgsCdkV7ka3sqYN3ZV+HgcgJC6rWOt7KD8ypAENz1t10+/lz3Varw\ncQZjsKX7AnPG+lTHSD3TMaJiwjShK4vBhyh+jT+jh+u5odQ5ncgK/SoazbRC+uKGiknX3f+Q2CYS\n8B8s9Df4YqF5/XQGIv81/BkiaK7PCh3rb33rmxI7iAWxMFbrBoTvaA1O1csTXV5JTBv/oQFE3QH2\nj5W0nyPV/cTEGGOMMYPBCxNjjDHGDAYvTIwxxhgzGLwwMcYYY8xguJD4tevUGTEDRU6O4ptIKAki\nvgzEfm2jQkES2pCLI5U6p5Ll5axfXvsE3FDHIFAKp9ouPHokoek1dYhtWxX1ni5B3Bd3F0RiGQhY\nS+jveKIl7VsQDRfgwtilWD+muEM+g5ZEnVdF14UuFq0WmjdUmjwWxMaumiGE0IJTa1WrmG46JgdT\nPeZoqvf12isquN476Iv95nPNpflc7/10rOc+OtCcO3qq59U24JLaap6X3TraTq/H8uxY91XpGCxg\nPDQbnQvaZg0xaBflpuRG4DzPYW5shpTn4Wuha+weSi62xZYuniSkJVFwqto9KzVnu2j+DiGE9bL/\nB4Z5q/d6f74nsWnQ/D+uVCQ9ynT+LsfaN4plRf8YTQM5Ad+FHfSNLVehGc3NKOKOvt/J5ZjuKQqf\nt1e/+omJMcYYYwaDFybGGGOMGQxemBhjjDFmMHhhYowxxpjBkF1EoJJl2YMQwicvrjvG/BV3u667\ndRUHdp6bl4xz3fxNISnXL7QwMcYYY4x5kfhVjjHGGGMGgxcmxhhjjBkMXpgYY4wxZjB4YWKMMcaY\nweCFiTHGGGMGgxcmxhhjjBkMXpgYY4wxZjB4YWKMMcaYweCFiTHGGGMGw/8BdXuovMzun0kAAAAA\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f91277bb400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_gallery(autoencoder(Variable(X_batch[:3, 1]).cuda())[0].cpu().data, 16, 16, n_row=1, n_col=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koloskov/anaconda3/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type Autoencoder16. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(autoencoder, \"autoencoder.people.16.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train stickmans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MSE loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "autoencoder = Autoencoder16().cuda()\n",
    "\n",
    "# Use Adam optimizer\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 59.144s\n",
      "  training loss (in-iteration): \t0.005758\n",
      "  validation loss: \t\t\t0.004257\n",
      "Epoch 2 of 100 took 59.719s\n",
      "  training loss (in-iteration): \t0.002830\n",
      "  validation loss: \t\t\t0.002145\n",
      "Epoch 3 of 100 took 60.002s\n",
      "  training loss (in-iteration): \t0.001663\n",
      "  validation loss: \t\t\t0.001522\n",
      "Epoch 4 of 100 took 59.701s\n",
      "  training loss (in-iteration): \t0.001251\n",
      "  validation loss: \t\t\t0.001207\n",
      "Epoch 5 of 100 took 58.549s\n",
      "  training loss (in-iteration): \t0.001062\n",
      "  validation loss: \t\t\t0.001050\n",
      "Epoch 6 of 100 took 59.574s\n",
      "  training loss (in-iteration): \t0.000949\n",
      "  validation loss: \t\t\t0.000952\n",
      "Epoch 7 of 100 took 58.296s\n",
      "  training loss (in-iteration): \t0.000873\n",
      "  validation loss: \t\t\t0.000880\n",
      "Epoch 8 of 100 took 59.160s\n",
      "  training loss (in-iteration): \t0.000813\n",
      "  validation loss: \t\t\t0.000836\n",
      "Epoch 9 of 100 took 58.278s\n",
      "  training loss (in-iteration): \t0.000766\n",
      "  validation loss: \t\t\t0.000794\n",
      "Epoch 10 of 100 took 58.561s\n",
      "  training loss (in-iteration): \t0.000726\n",
      "  validation loss: \t\t\t0.000741\n",
      "Epoch 11 of 100 took 58.872s\n",
      "  training loss (in-iteration): \t0.000693\n",
      "  validation loss: \t\t\t0.000707\n",
      "Epoch 12 of 100 took 57.722s\n",
      "  training loss (in-iteration): \t0.000665\n",
      "  validation loss: \t\t\t0.000679\n",
      "Epoch 13 of 100 took 60.637s\n",
      "  training loss (in-iteration): \t0.000641\n",
      "  validation loss: \t\t\t0.000651\n",
      "Epoch 14 of 100 took 59.729s\n",
      "  training loss (in-iteration): \t0.000620\n",
      "  validation loss: \t\t\t0.000634\n",
      "Epoch 15 of 100 took 58.776s\n",
      "  training loss (in-iteration): \t0.000601\n",
      "  validation loss: \t\t\t0.000615\n",
      "Epoch 16 of 100 took 57.987s\n",
      "  training loss (in-iteration): \t0.000585\n",
      "  validation loss: \t\t\t0.000601\n",
      "Epoch 17 of 100 took 58.457s\n",
      "  training loss (in-iteration): \t0.000569\n",
      "  validation loss: \t\t\t0.000586\n",
      "Epoch 18 of 100 took 59.336s\n",
      "  training loss (in-iteration): \t0.000556\n",
      "  validation loss: \t\t\t0.000574\n",
      "Epoch 19 of 100 took 61.311s\n",
      "  training loss (in-iteration): \t0.000543\n",
      "  validation loss: \t\t\t0.000561\n",
      "Epoch 20 of 100 took 58.381s\n",
      "  training loss (in-iteration): \t0.000532\n",
      "  validation loss: \t\t\t0.000547\n",
      "Epoch 21 of 100 took 59.234s\n",
      "  training loss (in-iteration): \t0.000520\n",
      "  validation loss: \t\t\t0.000531\n",
      "Epoch 22 of 100 took 60.811s\n",
      "  training loss (in-iteration): \t0.000510\n",
      "  validation loss: \t\t\t0.000522\n",
      "Epoch 23 of 100 took 69.610s\n",
      "  training loss (in-iteration): \t0.000501\n",
      "  validation loss: \t\t\t0.000511\n",
      "Epoch 24 of 100 took 68.581s\n",
      "  training loss (in-iteration): \t0.000491\n",
      "  validation loss: \t\t\t0.000502\n",
      "Epoch 25 of 100 took 70.596s\n",
      "  training loss (in-iteration): \t0.000483\n",
      "  validation loss: \t\t\t0.000493\n",
      "Epoch 26 of 100 took 60.485s\n",
      "  training loss (in-iteration): \t0.000474\n",
      "  validation loss: \t\t\t0.000481\n",
      "Epoch 27 of 100 took 58.963s\n",
      "  training loss (in-iteration): \t0.000467\n",
      "  validation loss: \t\t\t0.000473\n",
      "Epoch 28 of 100 took 59.808s\n",
      "  training loss (in-iteration): \t0.000459\n",
      "  validation loss: \t\t\t0.000466\n",
      "Epoch 29 of 100 took 60.878s\n",
      "  training loss (in-iteration): \t0.000451\n",
      "  validation loss: \t\t\t0.000459\n",
      "Epoch 30 of 100 took 60.580s\n",
      "  training loss (in-iteration): \t0.000444\n",
      "  validation loss: \t\t\t0.000452\n",
      "Epoch 31 of 100 took 59.119s\n",
      "  training loss (in-iteration): \t0.000437\n",
      "  validation loss: \t\t\t0.000447\n",
      "Epoch 32 of 100 took 58.986s\n",
      "  training loss (in-iteration): \t0.000431\n",
      "  validation loss: \t\t\t0.000441\n",
      "Epoch 33 of 100 took 58.137s\n",
      "  training loss (in-iteration): \t0.000426\n",
      "  validation loss: \t\t\t0.000435\n",
      "Epoch 34 of 100 took 58.252s\n",
      "  training loss (in-iteration): \t0.000421\n",
      "  validation loss: \t\t\t0.000431\n",
      "Epoch 35 of 100 took 59.431s\n",
      "  training loss (in-iteration): \t0.000416\n",
      "  validation loss: \t\t\t0.000428\n",
      "Epoch 36 of 100 took 60.225s\n",
      "  training loss (in-iteration): \t0.000411\n",
      "  validation loss: \t\t\t0.000421\n",
      "Epoch 37 of 100 took 61.045s\n",
      "  training loss (in-iteration): \t0.000407\n",
      "  validation loss: \t\t\t0.000418\n",
      "Epoch 38 of 100 took 60.124s\n",
      "  training loss (in-iteration): \t0.000402\n",
      "  validation loss: \t\t\t0.000413\n",
      "Epoch 39 of 100 took 60.799s\n",
      "  training loss (in-iteration): \t0.000398\n",
      "  validation loss: \t\t\t0.000411\n",
      "Epoch 40 of 100 took 60.376s\n",
      "  training loss (in-iteration): \t0.000394\n",
      "  validation loss: \t\t\t0.000409\n",
      "Epoch 41 of 100 took 60.562s\n",
      "  training loss (in-iteration): \t0.000391\n",
      "  validation loss: \t\t\t0.000407\n",
      "Epoch 42 of 100 took 67.683s\n",
      "  training loss (in-iteration): \t0.000387\n",
      "  validation loss: \t\t\t0.000405\n",
      "Epoch 43 of 100 took 71.354s\n",
      "  training loss (in-iteration): \t0.000384\n",
      "  validation loss: \t\t\t0.000401\n",
      "Epoch 44 of 100 took 72.090s\n",
      "  training loss (in-iteration): \t0.000381\n",
      "  validation loss: \t\t\t0.000398\n",
      "Epoch 45 of 100 took 63.516s\n",
      "  training loss (in-iteration): \t0.000378\n",
      "  validation loss: \t\t\t0.000393\n",
      "Epoch 46 of 100 took 58.196s\n",
      "  training loss (in-iteration): \t0.000375\n",
      "  validation loss: \t\t\t0.000392\n",
      "Epoch 47 of 100 took 59.751s\n",
      "  training loss (in-iteration): \t0.000372\n",
      "  validation loss: \t\t\t0.000388\n",
      "Epoch 48 of 100 took 59.772s\n",
      "  training loss (in-iteration): \t0.000369\n",
      "  validation loss: \t\t\t0.000389\n",
      "Epoch 49 of 100 took 61.389s\n",
      "  training loss (in-iteration): \t0.000366\n",
      "  validation loss: \t\t\t0.000385\n",
      "Epoch 50 of 100 took 59.186s\n",
      "  training loss (in-iteration): \t0.000364\n",
      "  validation loss: \t\t\t0.000384\n",
      "Epoch 51 of 100 took 59.377s\n",
      "  training loss (in-iteration): \t0.000362\n",
      "  validation loss: \t\t\t0.000379\n",
      "Epoch 52 of 100 took 59.363s\n",
      "  training loss (in-iteration): \t0.000360\n",
      "  validation loss: \t\t\t0.000380\n",
      "Epoch 53 of 100 took 59.224s\n",
      "  training loss (in-iteration): \t0.000358\n",
      "  validation loss: \t\t\t0.000375\n",
      "Epoch 54 of 100 took 59.395s\n",
      "  training loss (in-iteration): \t0.000356\n",
      "  validation loss: \t\t\t0.000370\n",
      "Epoch 55 of 100 took 58.838s\n",
      "  training loss (in-iteration): \t0.000354\n",
      "  validation loss: \t\t\t0.000363\n",
      "Epoch 56 of 100 took 61.125s\n",
      "  training loss (in-iteration): \t0.000353\n",
      "  validation loss: \t\t\t0.000360\n",
      "Epoch 57 of 100 took 60.002s\n",
      "  training loss (in-iteration): \t0.000351\n",
      "  validation loss: \t\t\t0.000359\n",
      "Epoch 58 of 100 took 58.470s\n",
      "  training loss (in-iteration): \t0.000350\n",
      "  validation loss: \t\t\t0.000356\n",
      "Epoch 59 of 100 took 58.802s\n",
      "  training loss (in-iteration): \t0.000347\n",
      "  validation loss: \t\t\t0.000354\n",
      "Epoch 60 of 100 took 59.071s\n",
      "  training loss (in-iteration): \t0.000346\n",
      "  validation loss: \t\t\t0.000353\n",
      "Epoch 61 of 100 took 58.337s\n",
      "  training loss (in-iteration): \t0.000345\n",
      "  validation loss: \t\t\t0.000351\n",
      "Epoch 62 of 100 took 59.461s\n",
      "  training loss (in-iteration): \t0.000343\n",
      "  validation loss: \t\t\t0.000349\n",
      "Epoch 63 of 100 took 58.137s\n",
      "  training loss (in-iteration): \t0.000341\n",
      "  validation loss: \t\t\t0.000348\n",
      "Epoch 64 of 100 took 60.416s\n",
      "  training loss (in-iteration): \t0.000339\n",
      "  validation loss: \t\t\t0.000346\n",
      "Epoch 65 of 100 took 60.107s\n",
      "  training loss (in-iteration): \t0.000338\n",
      "  validation loss: \t\t\t0.000346\n",
      "Epoch 66 of 100 took 60.042s\n",
      "  training loss (in-iteration): \t0.000337\n",
      "  validation loss: \t\t\t0.000344\n",
      "Epoch 67 of 100 took 60.217s\n",
      "  training loss (in-iteration): \t0.000335\n",
      "  validation loss: \t\t\t0.000344\n",
      "Epoch 68 of 100 took 58.397s\n",
      "  training loss (in-iteration): \t0.000334\n",
      "  validation loss: \t\t\t0.000343\n",
      "Epoch 69 of 100 took 58.366s\n",
      "  training loss (in-iteration): \t0.000333\n",
      "  validation loss: \t\t\t0.000342\n",
      "Epoch 70 of 100 took 59.465s\n",
      "  training loss (in-iteration): \t0.000331\n",
      "  validation loss: \t\t\t0.000340\n",
      "Epoch 71 of 100 took 59.000s\n",
      "  training loss (in-iteration): \t0.000330\n",
      "  validation loss: \t\t\t0.000339\n",
      "Epoch 72 of 100 took 58.433s\n",
      "  training loss (in-iteration): \t0.000329\n",
      "  validation loss: \t\t\t0.000338\n",
      "Epoch 73 of 100 took 61.337s\n",
      "  training loss (in-iteration): \t0.000328\n",
      "  validation loss: \t\t\t0.000338\n",
      "Epoch 74 of 100 took 61.023s\n",
      "  training loss (in-iteration): \t0.000327\n",
      "  validation loss: \t\t\t0.000337\n",
      "Epoch 75 of 100 took 61.262s\n",
      "  training loss (in-iteration): \t0.000325\n",
      "  validation loss: \t\t\t0.000337\n",
      "Epoch 76 of 100 took 61.072s\n",
      "  training loss (in-iteration): \t0.000325\n",
      "  validation loss: \t\t\t0.000336\n",
      "Epoch 77 of 100 took 59.221s\n",
      "  training loss (in-iteration): \t0.000324\n",
      "  validation loss: \t\t\t0.000336\n",
      "Epoch 78 of 100 took 59.002s\n",
      "  training loss (in-iteration): \t0.000322\n",
      "  validation loss: \t\t\t0.000335\n",
      "Epoch 79 of 100 took 58.384s\n",
      "  training loss (in-iteration): \t0.000322\n",
      "  validation loss: \t\t\t0.000335\n",
      "Epoch 80 of 100 took 60.537s\n",
      "  training loss (in-iteration): \t0.000321\n",
      "  validation loss: \t\t\t0.000334\n",
      "Epoch 81 of 100 took 61.067s\n",
      "  training loss (in-iteration): \t0.000320\n",
      "  validation loss: \t\t\t0.000333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82 of 100 took 59.786s\n",
      "  training loss (in-iteration): \t0.000318\n",
      "  validation loss: \t\t\t0.000331\n",
      "Epoch 83 of 100 took 60.923s\n",
      "  training loss (in-iteration): \t0.000318\n",
      "  validation loss: \t\t\t0.000330\n",
      "Epoch 84 of 100 took 60.707s\n",
      "  training loss (in-iteration): \t0.000317\n",
      "  validation loss: \t\t\t0.000329\n",
      "Epoch 85 of 100 took 71.307s\n",
      "  training loss (in-iteration): \t0.000316\n",
      "  validation loss: \t\t\t0.000328\n",
      "Epoch 86 of 100 took 91.216s\n",
      "  training loss (in-iteration): \t0.000315\n",
      "  validation loss: \t\t\t0.000327\n",
      "Epoch 87 of 100 took 99.512s\n",
      "  training loss (in-iteration): \t0.000314\n",
      "  validation loss: \t\t\t0.000325\n",
      "Epoch 88 of 100 took 90.834s\n",
      "  training loss (in-iteration): \t0.000313\n",
      "  validation loss: \t\t\t0.000326\n",
      "Epoch 89 of 100 took 68.913s\n",
      "  training loss (in-iteration): \t0.000312\n",
      "  validation loss: \t\t\t0.000324\n",
      "Epoch 90 of 100 took 60.701s\n",
      "  training loss (in-iteration): \t0.000312\n",
      "  validation loss: \t\t\t0.000323\n",
      "Epoch 91 of 100 took 61.179s\n",
      "  training loss (in-iteration): \t0.000311\n",
      "  validation loss: \t\t\t0.000323\n",
      "Epoch 92 of 100 took 61.665s\n",
      "  training loss (in-iteration): \t0.000311\n",
      "  validation loss: \t\t\t0.000322\n",
      "Epoch 93 of 100 took 62.194s\n",
      "  training loss (in-iteration): \t0.000310\n",
      "  validation loss: \t\t\t0.000321\n",
      "Epoch 94 of 100 took 60.158s\n",
      "  training loss (in-iteration): \t0.000309\n",
      "  validation loss: \t\t\t0.000320\n",
      "Epoch 95 of 100 took 60.583s\n",
      "  training loss (in-iteration): \t0.000308\n",
      "  validation loss: \t\t\t0.000318\n",
      "Epoch 96 of 100 took 61.862s\n",
      "  training loss (in-iteration): \t0.000307\n",
      "  validation loss: \t\t\t0.000318\n",
      "Epoch 97 of 100 took 59.768s\n",
      "  training loss (in-iteration): \t0.000306\n",
      "  validation loss: \t\t\t0.000318\n",
      "Epoch 98 of 100 took 59.291s\n",
      "  training loss (in-iteration): \t0.000306\n",
      "  validation loss: \t\t\t0.000316\n",
      "Epoch 99 of 100 took 61.159s\n",
      "  training loss (in-iteration): \t0.000305\n",
      "  validation loss: \t\t\t0.000317\n",
      "Epoch 100 of 100 took 86.324s\n",
      "  training loss (in-iteration): \t0.000305\n",
      "  validation loss: \t\t\t0.000315\n"
     ]
    }
   ],
   "source": [
    "# Train your autoencoder\n",
    "# Visualize progress in reconstruction and loss decay\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "reconstructed_pictures = []\n",
    "\n",
    "import time\n",
    "num_epochs = 100 # total amount of full passes over training data\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    start_time = time.time()\n",
    "    autoencoder.train(True) # enable dropout / batch_norm training behavior\n",
    "    i = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        i += 1\n",
    "        # train on batch\n",
    "        X_batch_0 = torch.FloatTensor(y_batch[:, 0])\n",
    "        X_batch_0 = Variable(X_batch_0).cuda()\n",
    "#         X_batch = Variable(X_batch)\n",
    "#         y_batch = Variable(y_batch)\n",
    "        output_img, _ = autoencoder(X_batch_0)\n",
    "        loss = criterion(output_img, X_batch_0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.append(loss.cpu().data.numpy()[0])\n",
    "#         train_loss.append(loss.data.numpy())\n",
    "        \n",
    "        X_batch_1 = torch.FloatTensor(y_batch[:, 1])\n",
    "        X_batch_1 = Variable(X_batch_1).cuda()\n",
    "#         X_batch = Variable(X_batch)\n",
    "#         y_batch = Variable(y_batch)\n",
    "        output_img, _ = autoencoder(X_batch_1)\n",
    "        loss = criterion(output_img, X_batch_1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.append(loss.cpu().data.numpy()[0])\n",
    "#         train_loss.append(loss.data.numpy())\n",
    "#         print(i, \":\", loss.data.cpu().numpy()[0], end=\", \")\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    autoencoder.train(False) # disable dropout / use averages for batch_norm\n",
    "    for X_batch, y_batch in val_loader:\n",
    "        X_batch_0 = Variable(y_batch[:, 0]).cuda()\n",
    "        output_img, _ = autoencoder(X_batch_0)\n",
    "        val_loss.append(criterion(output_img, X_batch_0).cpu().data.numpy()[0])\n",
    "        X_batch_1 = Variable(y_batch[:, 1]).cuda()\n",
    "        output_img, _ = autoencoder(X_batch_1)\n",
    "        val_loss.append(criterion(output_img, X_batch_1).cpu().data.numpy()[0])\n",
    "#         val_loss.append(criterion(output_img, X_batch).data.numpy())\n",
    "#     if epoch % 16 == 0:\n",
    "#         X_batch = Variable(torch.FloatTensor(np.array([X_val[247]]))).cuda()\n",
    "#         output_img, _ = autoencoder(X_batch)\n",
    "#         # reconstructed_pictures.append(output_img.cpu().data.numpy()[0])\n",
    "#         reconstructed_pictures.append(output_img.data.numpy())\n",
    "    # Then we print the results for this epoch:\n",
    "    print \n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(train_loss[-2 * len(train_loader):])))\n",
    "    print(\"  validation loss: \\t\\t\\t{:.6f}\".format(\n",
    "        np.mean(val_loss[-2 * len(val_loader):])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koloskov/anaconda3/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type Autoencoder16. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(autoencoder, \"autoencoder.stickmen.16.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_gallery(y_batch[:3, 1], image_h, image_w, n_row=1, n_col=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_gallery(autoencoder(Variable(y_batch[:3, 1]).cuda())[0].cpu().data, image_h, image_w, n_row=1, n_col=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
