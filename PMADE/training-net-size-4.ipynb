{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "from loader import get_train_loader\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Autoencoder4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder4, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=2, padding=0),\n",
    "#             nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16, 16, 2, stride=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 3, kernel_size=3, padding=1),\n",
    "            \n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        latent_code = self.encoder(x)\n",
    "        reconstruction = self.decoder(latent_code)\n",
    "        return reconstruction, latent_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder_people = torch.load('autoencoder.people.4.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder_stickmans = torch.load('autoencoder.stickmen.4.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim_x = 8\n",
    "batch_size = 64\n",
    "train_loader, val_loader = get_train_loader(\"../deepfashion/index.p\", batch_size=batch_size, resize_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net4Group2(nn.Module):\n",
    "    \"\"\"\n",
    "        Network for predictions group 2 images based on group 1.\n",
    "        input size: 4x4\n",
    "        output size: 4x4\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers=[1], bottelneck_size=32):\n",
    "        super(Net4Group2, self).__init__()\n",
    "        \n",
    "        self.first_part = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=2, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(8, 16, kernel_size=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.up_1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(48, 64, 1, stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv_1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=(2, 2), stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=(2, 2), stride=1, padding=0),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 3, kernel_size=(2, 2), stride=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, keypoints, embeddings):\n",
    "        x = self.first_part(x)\n",
    "        x = torch.cat((x, keypoints, embeddings), dim=1)\n",
    "        x = self.up_1(x)\n",
    "        x = self.conv_1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask_input_1 = np.zeros((batch_size, 3, dim_x, dim_x)).astype(bool)\n",
    "for m in range(batch_size):\n",
    "    for n in range(3):\n",
    "        for i in range(dim_x):\n",
    "            for j in range(dim_x):\n",
    "                if i % 2 == 0 and j % 2 ==0:\n",
    "                    mask_input_1[m][n][i][j] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask_output_1 = np.zeros((batch_size, 3, dim_x, dim_x)).astype(bool)\n",
    "for m in range(batch_size):\n",
    "    for n in range(3):\n",
    "        for i in range(dim_x):\n",
    "            for j in range(dim_x):\n",
    "                if i % 2 == 0 and j % 2 == 1:\n",
    "                    mask_output_1[m][n][i][j] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "net4_group2 = Net4Group2().cuda()\n",
    "optimizer = optim.Adam(net4_group2.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network(net, criterion, optimizer, mask_input, mask_output, shape_input, shape_output, num_epochs):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        net.train(True)\n",
    "        i = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            i += 1\n",
    "            if X_batch.shape[0] != batch_size:\n",
    "                continue\n",
    "            X_batch_input = X_batch[:, 0]\n",
    "            X_batch_input = X_batch_input[mask_input.nonzero()].view(shape_input)\n",
    "            X_batch_input = Variable(X_batch_input).cuda()\n",
    "\n",
    "            X_batch_output = X_batch[:, 1]\n",
    "            X_batch_output = X_batch_output[mask_output.nonzero()].view(shape_output)\n",
    "            X_batch_output = Variable(X_batch_output).cuda()\n",
    "\n",
    "            output_img = net(X_batch_input,\n",
    "                                     keypoints=autoencoder_stickmans(Variable(y_batch[:, 1]).cuda())[1],\n",
    "                                     embeddings=autoencoder_people(Variable(X_batch[:, 0]).cuda())[1])\n",
    "            loss = criterion(output_img, X_batch_output)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.append(loss.cpu().data.numpy()[0])\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "    #     autoencoder.train(False) # disable dropout / use averages for batch_norm\n",
    "    #     for X_batch, y_batch in val_loader:\n",
    "    #         X_batch_0 = Variable(y_batch[:, 0]).cuda()\n",
    "    #         output_img, _ = autoencoder(X_batch_0)\n",
    "    #         val_loss.append(criterion(output_img, X_batch_0).cpu().data.numpy()[0])\n",
    "    #         X_batch_1 = Variable(y_batch[:, 1]).cuda()\n",
    "    #         output_img, _ = autoencoder(X_batch_1)\n",
    "    #         val_loss.append(criterion(output_img, X_batch_1).cpu().data.numpy()[0])\n",
    "\n",
    "        print \n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "            np.mean(train_loss[-2 * len(train_loader):])))\n",
    "    #     print(\"  validation loss: \\t\\t\\t{:.6f}\".format(\n",
    "    #         np.mean(val_loss[-2 * len(val_loader):])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 45.934s\n",
      "  training loss (in-iteration): \t0.018011\n",
      "Epoch 2 of 100 took 44.553s\n",
      "  training loss (in-iteration): \t0.012722\n",
      "Epoch 3 of 100 took 44.233s\n",
      "  training loss (in-iteration): \t0.007005\n",
      "Epoch 4 of 100 took 44.253s\n",
      "  training loss (in-iteration): \t0.006341\n",
      "Epoch 5 of 100 took 44.978s\n",
      "  training loss (in-iteration): \t0.005992\n",
      "Epoch 6 of 100 took 42.923s\n",
      "  training loss (in-iteration): \t0.005762\n",
      "Epoch 7 of 100 took 42.849s\n",
      "  training loss (in-iteration): \t0.005590\n",
      "Epoch 8 of 100 took 43.588s\n",
      "  training loss (in-iteration): \t0.005446\n",
      "Epoch 9 of 100 took 44.872s\n",
      "  training loss (in-iteration): \t0.005321\n",
      "Epoch 10 of 100 took 45.248s\n",
      "  training loss (in-iteration): \t0.005213\n",
      "Epoch 11 of 100 took 45.352s\n",
      "  training loss (in-iteration): \t0.005118\n",
      "Epoch 12 of 100 took 43.590s\n",
      "  training loss (in-iteration): \t0.005033\n",
      "Epoch 13 of 100 took 48.816s\n",
      "  training loss (in-iteration): \t0.004957\n",
      "Epoch 14 of 100 took 50.339s\n",
      "  training loss (in-iteration): \t0.004889\n",
      "Epoch 15 of 100 took 49.096s\n",
      "  training loss (in-iteration): \t0.004825\n",
      "Epoch 16 of 100 took 47.169s\n",
      "  training loss (in-iteration): \t0.004764\n",
      "Epoch 17 of 100 took 45.707s\n",
      "  training loss (in-iteration): \t0.004703\n",
      "Epoch 18 of 100 took 43.022s\n",
      "  training loss (in-iteration): \t0.004646\n",
      "Epoch 19 of 100 took 44.418s\n",
      "  training loss (in-iteration): \t0.004589\n",
      "Epoch 20 of 100 took 43.855s\n",
      "  training loss (in-iteration): \t0.004535\n",
      "Epoch 21 of 100 took 42.683s\n",
      "  training loss (in-iteration): \t0.004488\n",
      "Epoch 22 of 100 took 42.461s\n",
      "  training loss (in-iteration): \t0.004442\n",
      "Epoch 23 of 100 took 43.200s\n",
      "  training loss (in-iteration): \t0.004394\n",
      "Epoch 24 of 100 took 46.332s\n",
      "  training loss (in-iteration): \t0.004345\n",
      "Epoch 25 of 100 took 46.449s\n",
      "  training loss (in-iteration): \t0.004300\n",
      "Epoch 26 of 100 took 44.430s\n",
      "  training loss (in-iteration): \t0.004258\n",
      "Epoch 27 of 100 took 45.340s\n",
      "  training loss (in-iteration): \t0.004220\n",
      "Epoch 28 of 100 took 45.723s\n",
      "  training loss (in-iteration): \t0.004185\n",
      "Epoch 29 of 100 took 48.219s\n",
      "  training loss (in-iteration): \t0.004150\n",
      "Epoch 30 of 100 took 47.950s\n",
      "  training loss (in-iteration): \t0.004113\n",
      "Epoch 31 of 100 took 47.548s\n",
      "  training loss (in-iteration): \t0.004080\n",
      "Epoch 32 of 100 took 47.504s\n",
      "  training loss (in-iteration): \t0.004051\n",
      "Epoch 33 of 100 took 47.486s\n",
      "  training loss (in-iteration): \t0.004020\n",
      "Epoch 34 of 100 took 47.495s\n",
      "  training loss (in-iteration): \t0.003990\n",
      "Epoch 35 of 100 took 44.627s\n",
      "  training loss (in-iteration): \t0.003964\n",
      "Epoch 36 of 100 took 43.801s\n",
      "  training loss (in-iteration): \t0.003943\n",
      "Epoch 37 of 100 took 43.611s\n",
      "  training loss (in-iteration): \t0.003921\n",
      "Epoch 38 of 100 took 42.393s\n",
      "  training loss (in-iteration): \t0.003894\n",
      "Epoch 39 of 100 took 44.253s\n",
      "  training loss (in-iteration): \t0.003870\n",
      "Epoch 40 of 100 took 43.048s\n",
      "  training loss (in-iteration): \t0.003848\n",
      "Epoch 41 of 100 took 43.872s\n",
      "  training loss (in-iteration): \t0.003822\n",
      "Epoch 42 of 100 took 43.859s\n",
      "  training loss (in-iteration): \t0.003793\n",
      "Epoch 43 of 100 took 43.133s\n",
      "  training loss (in-iteration): \t0.003768\n",
      "Epoch 44 of 100 took 44.585s\n",
      "  training loss (in-iteration): \t0.003747\n",
      "Epoch 45 of 100 took 44.113s\n",
      "  training loss (in-iteration): \t0.003729\n",
      "Epoch 46 of 100 took 45.417s\n",
      "  training loss (in-iteration): \t0.003716\n",
      "Epoch 47 of 100 took 43.732s\n",
      "  training loss (in-iteration): \t0.003706\n",
      "Epoch 48 of 100 took 44.176s\n",
      "  training loss (in-iteration): \t0.003697\n",
      "Epoch 49 of 100 took 44.489s\n",
      "  training loss (in-iteration): \t0.003685\n",
      "Epoch 50 of 100 took 43.516s\n",
      "  training loss (in-iteration): \t0.003666\n",
      "Epoch 51 of 100 took 48.736s\n",
      "  training loss (in-iteration): \t0.003643\n",
      "Epoch 52 of 100 took 47.533s\n",
      "  training loss (in-iteration): \t0.003617\n",
      "Epoch 53 of 100 took 47.922s\n",
      "  training loss (in-iteration): \t0.003597\n",
      "Epoch 54 of 100 took 46.871s\n",
      "  training loss (in-iteration): \t0.003578\n",
      "Epoch 55 of 100 took 50.494s\n",
      "  training loss (in-iteration): \t0.003552\n",
      "Epoch 56 of 100 took 43.142s\n",
      "  training loss (in-iteration): \t0.003533\n",
      "Epoch 57 of 100 took 43.900s\n",
      "  training loss (in-iteration): \t0.003522\n",
      "Epoch 58 of 100 took 43.851s\n",
      "  training loss (in-iteration): \t0.003503\n",
      "Epoch 59 of 100 took 43.691s\n",
      "  training loss (in-iteration): \t0.003476\n",
      "Epoch 60 of 100 took 44.267s\n",
      "  training loss (in-iteration): \t0.003457\n",
      "Epoch 61 of 100 took 44.222s\n",
      "  training loss (in-iteration): \t0.003449\n",
      "Epoch 62 of 100 took 45.986s\n",
      "  training loss (in-iteration): \t0.003437\n",
      "Epoch 63 of 100 took 45.169s\n",
      "  training loss (in-iteration): \t0.003424\n",
      "Epoch 64 of 100 took 45.171s\n",
      "  training loss (in-iteration): \t0.003413\n",
      "Epoch 65 of 100 took 44.675s\n",
      "  training loss (in-iteration): \t0.003399\n",
      "Epoch 66 of 100 took 43.937s\n",
      "  training loss (in-iteration): \t0.003385\n",
      "Epoch 67 of 100 took 45.280s\n",
      "  training loss (in-iteration): \t0.003371\n",
      "Epoch 68 of 100 took 53.496s\n",
      "  training loss (in-iteration): \t0.003358\n",
      "Epoch 69 of 100 took 60.471s\n",
      "  training loss (in-iteration): \t0.003341\n",
      "Epoch 70 of 100 took 51.772s\n",
      "  training loss (in-iteration): \t0.003324\n",
      "Epoch 71 of 100 took 45.938s\n",
      "  training loss (in-iteration): \t0.003308\n",
      "Epoch 72 of 100 took 46.535s\n",
      "  training loss (in-iteration): \t0.003294\n",
      "Epoch 73 of 100 took 45.556s\n",
      "  training loss (in-iteration): \t0.003284\n",
      "Epoch 74 of 100 took 44.670s\n",
      "  training loss (in-iteration): \t0.003280\n",
      "Epoch 75 of 100 took 44.811s\n",
      "  training loss (in-iteration): \t0.003282\n",
      "Epoch 76 of 100 took 42.654s\n",
      "  training loss (in-iteration): \t0.003284\n",
      "Epoch 77 of 100 took 42.567s\n",
      "  training loss (in-iteration): \t0.003288\n",
      "Epoch 78 of 100 took 42.509s\n",
      "  training loss (in-iteration): \t0.003294\n",
      "Epoch 79 of 100 took 42.736s\n",
      "  training loss (in-iteration): \t0.003286\n",
      "Epoch 80 of 100 took 48.866s\n",
      "  training loss (in-iteration): \t0.003265\n",
      "Epoch 81 of 100 took 55.565s\n",
      "  training loss (in-iteration): \t0.003240\n",
      "Epoch 82 of 100 took 52.493s\n",
      "  training loss (in-iteration): \t0.003214\n",
      "Epoch 83 of 100 took 45.654s\n",
      "  training loss (in-iteration): \t0.003191\n",
      "Epoch 84 of 100 took 43.505s\n",
      "  training loss (in-iteration): \t0.003169\n",
      "Epoch 85 of 100 took 43.194s\n",
      "  training loss (in-iteration): \t0.003152\n",
      "Epoch 86 of 100 took 43.713s\n",
      "  training loss (in-iteration): \t0.003141\n",
      "Epoch 87 of 100 took 42.924s\n",
      "  training loss (in-iteration): \t0.003131\n",
      "Epoch 88 of 100 took 43.571s\n",
      "  training loss (in-iteration): \t0.003124\n",
      "Epoch 89 of 100 took 43.002s\n",
      "  training loss (in-iteration): \t0.003117\n",
      "Epoch 90 of 100 took 44.293s\n",
      "  training loss (in-iteration): \t0.003106\n",
      "Epoch 91 of 100 took 43.255s\n",
      "  training loss (in-iteration): \t0.003092\n",
      "Epoch 92 of 100 took 43.858s\n",
      "  training loss (in-iteration): \t0.003083\n",
      "Epoch 93 of 100 took 42.961s\n",
      "  training loss (in-iteration): \t0.003076\n",
      "Epoch 94 of 100 took 44.567s\n",
      "  training loss (in-iteration): \t0.003071\n",
      "Epoch 95 of 100 took 43.869s\n",
      "  training loss (in-iteration): \t0.003066\n",
      "Epoch 96 of 100 took 43.810s\n",
      "  training loss (in-iteration): \t0.003061\n",
      "Epoch 97 of 100 took 45.509s\n",
      "  training loss (in-iteration): \t0.003052\n",
      "Epoch 98 of 100 took 48.841s\n",
      "  training loss (in-iteration): \t0.003038\n",
      "Epoch 99 of 100 took 51.199s\n",
      "  training loss (in-iteration): \t0.003021\n",
      "Epoch 100 of 100 took 49.292s\n",
      "  training loss (in-iteration): \t0.003006\n"
     ]
    }
   ],
   "source": [
    "train_network(net4_group2, criterion, optimizer, mask_input_1, mask_output_1,\n",
    "              shape_input=(batch_size, 3, int(dim_x / 2), int(dim_x / 2)),\n",
    "              shape_output=(batch_size, 3, int(dim_x / 2), int(dim_x / 2)),\n",
    "              num_epochs=100\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koloskov/anaconda3/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type Net4Group2. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(net4_group2, \"net4.group2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_gallery(images, h, w, n_row=3, n_col=6):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    scale_const = 1.2\n",
    "    plt.figure(figsize=(3 / scale_const * n_col, 3.4 / scale_const * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].numpy().transpose(1,2,0), cmap=plt.cm.gray, vmin=-1, vmax=1, interpolation='nearest')\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for X_batch, y_batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_batch_input = X_batch[:, 0]\n",
    "X_batch_input = X_batch_input[mask_input.nonzero()].view((batch_size, 3, int(dim_x / 2), int(dim_x / 2)))\n",
    "X_batch_input = Variable(X_batch_input).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAACxCAYAAADwMnaUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAABStJREFUeJzt2bFrnHUcx/F7ctfgYrHN1aAc3k2K\ngyAq6FBRdHBwdNHBwcE/Q/wXHPwHxN3RIQiW1lICCglaLDpIKiImjbUKVaLH46RTf/bOJH4/mNdr\nPvh9OH489+a5ru/7AQBAgpXqAQAAfxEmAEAMYQIAxBAmAEAMYQIAxBAmAEAMYQIAxBAmAEAMYQIA\nxBAmAECM0TIfHo/H/Ww2O6Yph/P7wUH1hKZbu99VT2g6++BD1RPu6PrO9cGN/f2u4uy1tbV+MplU\nHH1XK13JV7KQm/v71ROazozH1ROatra2bvR9f67i7PF43E+n04qj72o+n1dPaJr/9kv1hKbhPfdW\nT2ja3t5e6K4vFSaz2Wywubn571cdo93vv62e0PThO29VT2h67e13qyfc0fnnni87ezKZDDY2NsrO\n/yerK7kvOT94/73qCU2vvPFm9YSmM2fv26k6ezqdxj7Tf/7pVvWEph+/vFA9oen0I89WT2haX19f\n6K7nPuUAgBNHmAAAMYQJABBDmAAAMYQJABBDmAAAMYQJABBDmAAAMYQJABBDmAAAMYQJABBDmAAA\nMYQJABBDmAAAMYQJABBDmAAAMYQJABBDmAAAMYQJABBDmAAAMYQJABBDmAAAMYQJABBDmAAAMYQJ\nABBDmAAAMYQJABBDmAAAMYQJABBDmAAAMYQJABBDmAAAMYQJABBDmAAAMYQJABBDmAAAMUbVA47K\nN199UT2haf2BcfWEtq56AMvoRsPqCU2XP/2sekLTy6++Xj2BJQ1P5f48DecH1ROaRqNT1RMOzRsT\nACCGMAEAYggTACCGMAEAYggTACCGMAEAYggTACCGMAEAYggTACCGMAEAYggTACCGMAEAYggTACCG\nMAEAYggTACCGMAEAYggTACCGMAEAYggTACCGMAEAYggTACCGMAEAYggTACCGMAEAYggTACCGMAEA\nYggTACCGMAEAYggTACCGMAEAYggTACCGMAEAYggTACCGMAEAYggTACCGMAEAYggTACCGMAEAYoyq\nBxyVvd0fqic0fbx7f/WEpheHq9UTGrrqAZGuXfu6ekLThU+uVE9ounTpYvUEljT/9Xb1hKbV0enq\nCU3d/+DR6Y0JABBDmAAAMYQJABBDmAAAMYQJABBDmAAAMYQJABBDmAAAMYQJABBDmAAAMYQJABBD\nmAAAMYQJABBDmAAAMYQJABBDmAAAMYQJABBDmAAAMYQJABBDmAAAMYQJABBDmAAAMYQJABBDmAAA\nMYQJABBDmAAAMYQJABBDmAAAMYQJABBDmAAAMYQJABBDmAAAMYQJABBDmAAAMYQJABBDmAAAMYQJ\nABBDmAAAMUbVA47KH31uY710/rHqCU1931dPYAlXP79aPaHp4cefrp7Q9MSTT1VPYElXLn5UPaHp\n9s296glNLzz6TPWEQ8v9NQcAThxhAgDEECYAQAxhAgDEECYAQAxhAgDEECYAQAxhAgDEECYAQAxh\nAgDEECYAQAxhAgDEECYAQAxhAgDEECYAQAxhAgDEECYAQAxhAgDEECYAQAxhAgDEECYAQAxhAgDE\nECYAQAxhAgDEECYAQAxhAgDEECYAQAxhAgDEECYAQAxhAgDEECYAQAxhAgDEECYAQAxhAgDEECYA\nQAxhAgDEECYAQAxhAgDE6Pq+X/zDXbc3GAx2jm8O/G3a9/25ioPdc/5j7jonxUJ3fakwAQA4Tv7K\nAQBiCBMAIIYwAQBiCBMAIIYwAQBiCBMAIIYwAQBiCBMAIIYwAQBi/Am4TX9Jr/FxwgAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f273e27e6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_gallery(X_batch_input.cpu().data, 8, 8, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAACxCAYAAADwMnaUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAABW1JREFUeJzt2T2LnFUcxuFnXrKbEJVMJkEIZgbB\nl0rstFNMl0Kw8SvYiqCWKa219bMIMSIJCMJaiFhmU8ZUMdk1a+ZYabVn3XGY/G/wuuoHzs3u2Yff\nzoxaawMAQIJx9QAAgL8JEwAghjABAGIIEwAghjABAGIIEwAghjABAGIIEwAghjABAGIIEwAgxnSd\nh+fzeVsuFtvaspGDRw+rJ3Sd2T1XPaFrMp1UTzjW/v694cGDB6OKsy9dutSWy2XF0f/q8NHv1RO6\npjs71RO6xpO1XnXP1N7e3m+ttcsVZ8/n87YIfaf/eRh813fPV0/oK3lrns7e3k+nuutr/bUuF4vh\n1q1b/33VFv1855vqCV0vvfZG9YSuFy7Mqycc691r18rOXi6Xw+3bd8rOP8mvP35fPaFrfuXl6gld\n5y/Mqid0zWazu1VnLxaL4dubN6uOP9H9X4Lv+qtvV0/oGof+szkMw3BhNj/VXfdVDgAQQ5gAADGE\nCQAQQ5gAADGECQAQQ5gAADGECQAQQ5gAADGECQAQQ5gAADGECQAQQ5gAADGECQAQQ5gAADGECQAQ\nQ5gAADGECQAQQ5gAADGECQAQQ5gAADGECQAQQ5gAADGECQAQQ5gAADGECQAQQ5gAADGECQAQQ5gA\nADGECQAQQ5gAADGECQAQQ5gAADGECQAQQ5gAADGECQAQQ5gAADGm6zy8aqvh8PBgW1s28sN3N6sn\ndJ3dnVRP6Hp+9k71hI5R2cmttWG1elp2/klufPFl9YSu965fr57Q9eEH71dPiDQajYbxNPP91A4P\nqyd0jY4eV0/oWo2fq56wMZ+YAAAxhAkAEEOYAAAxhAkAEEOYAAAxhAkAEEOYAAAxhAkAEEOYAAAx\nhAkAEEOYAAAxhAkAEEOYAAAxhAkAEEOYAAAxhAkAEEOYAAAxhAkAEEOYAAAxhAkAEEOYAAAxhAkA\nEEOYAAAxhAkAEEOYAAAxhAkAEEOYAAAxhAkAEEOYAAAxhAkAEEOYAAAxhAkAEEOYAAAxhAkAEEOY\nAAAxhAkAEEOYAAAxhAkAEGO6zsOr1Wo4OHi8rS0bubP/R/WErtcPdqsndF1dteoJx2qtbldrbXhy\n9KTs/JPcvbdfPaHr7JD5MxuGYTizc656QqbWhiH0HbDbct+b7eBh9YSutnO+esLGfGICAMQQJgBA\nDGECAMQQJgBADGECAMQQJgBADGECAMQQJgBADGECAMQQJgBADGECAMQQJgBADGECAMQQJgBADGEC\nAMQQJgBADGECAMQQJgBADGECAMQQJgBADGECAMQQJgBADGECAMQQJgBADGECAMQQJgBADGECAMQQ\nJgBADGECAMQQJgBADGECAMQQJgBADGECAMQQJgBADGECAMQQJgBADGECAMQQJgBAjOlaT7dhGK2e\nbmnKZj79+KPqCV0XL86qJ3SNJ5PqCccajUal57dWenzXK2++VT2ha/7ileoJXZNx6C+02Kq14cnR\nUfWMY9346uvqCV2fff5J9YSuq/Or1RM25hMTACCGMAEAYggTACCGMAEAYggTACCGMAEAYggTACCG\nMAEAYggTACCGMAEAYggTACCGMAEAYggTACCGMAEAYggTACCGMAEAYggTACCGMAEAYggTACCGMAEA\nYggTACCGMAEAYggTACCGMAEAYggTACCGMAEAYggTACCGMAEAYggTACCGMAEAYggTACCGMAEAYggT\nACCGMAEAYggTACCGMAEAYggTACCGMAEAYoxaa6d/eDS6PwzD3e3NgX8sW2uXKw52z3nG3HX+L051\n19cKEwCAbfJVDgAQQ5gAADGECQAQQ5gAADGECQAQQ5gAADGECQAQQ5gAADGECQAQ4y9WqJfnF+VZ\nYAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f27500bfef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_gallery(net4_group2(X_batch_input,\n",
    "                 keypoints=autoencoder_stickmans(Variable(y_batch[:, 1]).cuda())[1],\n",
    "                 embeddings=autoencoder_people(Variable(X_batch[:, 0]).cuda())[1]).cpu().data,\n",
    "             8, 8, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_batch_output = X_batch[:, 1]\n",
    "X_batch_output = X_batch_output[mask_output.nonzero()].view((batch_size, 3, int(dim_x / 2), int(dim_x / 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAACxCAYAAADwMnaUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f273e2f82e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_gallery(X_batch_output, 8, 8, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net4Group3(nn.Module):\n",
    "    \"\"\"\n",
    "        Network for predictions group 2 images based on group 1.\n",
    "        input size: 4x4\n",
    "        output size: 4x4\n",
    "    \"\"\"\n",
    "    def __init__(self, layers=[1], bottelneck_size=32):\n",
    "        super(Net4Group3, self).__init__()\n",
    "        \n",
    "        self.first_part = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(3, 8, kernel_size=2, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(8, 8, kernel_size=(1, 3), stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(8, 16, kernel_size=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 16, kernel_size=(1, 3)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 16, kernel_size=3),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.up_1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(48, 64, 1, stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv_1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=(2, 2), stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=(2, 2), stride=1, padding=0),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 3, kernel_size=(2, 2), stride=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, keypoints, embeddings):\n",
    "        x = self.first_part(x)\n",
    "#         print(x.shape)\n",
    "        x = torch.cat((x, keypoints, embeddings), dim=1)\n",
    "        x = self.up_1(x)\n",
    "        x = self.conv_1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class Net4Group3(nn.Module):\n",
    "#     \"\"\"\n",
    "#         Network for predictions group 2 images based on group 1.\n",
    "#         input size: 4x4\n",
    "#         output size: 4x4\n",
    "#     \"\"\"\n",
    "#     def __init__(self, layers=[1], bottelneck_size=32):\n",
    "#         super(Net4Group3, self).__init__()\n",
    "        \n",
    "#         self.first_part = nn.Sequential(\n",
    "#             nn.Conv2d(3, 8, kernel_size=2),\n",
    "#             nn.MaxPool2d((1, 2)),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(8, 16, kernel_size=2, stride=1, padding=1),\n",
    "#             nn.BatchNorm2d(16),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "\n",
    "#         self.up_1 = nn.Sequential(\n",
    "#             nn.ConvTranspose2d(48, 64, 1, stride=2)\n",
    "#         )\n",
    "\n",
    "#         self.conv_1 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 64, kernel_size=(2, 2), stride=1, padding=0),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(64, 64, kernel_size=(2, 2), stride=1, padding=0),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(64, 3, kernel_size=(2, 2), stride=1, padding=0),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x, keypoints, embeddings):\n",
    "# #         print(x.shape)\n",
    "#         x = self.first_part(x)\n",
    "# #         print(x.shape)\n",
    "#         x = torch.cat((x, keypoints, embeddings), dim=1)\n",
    "#         x = self.up_1(x)\n",
    "#         x = self.conv_1(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask_input_2 = np.zeros((batch_size, 3, dim_x, dim_x)).astype(bool)\n",
    "for m in range(batch_size):\n",
    "    for n in range(3):\n",
    "        for i in range(dim_x):\n",
    "            for j in range(dim_x):\n",
    "                if i % 2 == 0:\n",
    "                    mask_input_2[m][n][i][j] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask_output_2 = np.zeros((batch_size, 3, dim_x, dim_x)).astype(bool)\n",
    "for m in range(batch_size):\n",
    "    for n in range(3):\n",
    "        for i in range(dim_x):\n",
    "            for j in range(dim_x):\n",
    "                if i % 2 == 1 and j % 2 == 0:\n",
    "                    mask_output_2[m][n][i][j] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "net4_group3 = Net4Group3().cuda()\n",
    "optimizer = optim.Adam(net4_group3.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 43.915s\n",
      "  training loss (in-iteration): \t0.016668\n",
      "Epoch 2 of 100 took 43.274s\n",
      "  training loss (in-iteration): \t0.012164\n",
      "Epoch 3 of 100 took 43.242s\n",
      "  training loss (in-iteration): \t0.007289\n",
      "Epoch 4 of 100 took 43.849s\n",
      "  training loss (in-iteration): \t0.006705\n",
      "Epoch 5 of 100 took 43.894s\n",
      "  training loss (in-iteration): \t0.006379\n",
      "Epoch 6 of 100 took 43.236s\n",
      "  training loss (in-iteration): \t0.006155\n",
      "Epoch 7 of 100 took 44.300s\n",
      "  training loss (in-iteration): \t0.005981\n",
      "Epoch 8 of 100 took 44.871s\n",
      "  training loss (in-iteration): \t0.005840\n",
      "Epoch 9 of 100 took 43.848s\n",
      "  training loss (in-iteration): \t0.005720\n",
      "Epoch 10 of 100 took 43.909s\n",
      "  training loss (in-iteration): \t0.005613\n",
      "Epoch 11 of 100 took 45.354s\n",
      "  training loss (in-iteration): \t0.005518\n",
      "Epoch 12 of 100 took 44.739s\n",
      "  training loss (in-iteration): \t0.005432\n",
      "Epoch 13 of 100 took 43.520s\n",
      "  training loss (in-iteration): \t0.005351\n",
      "Epoch 14 of 100 took 43.197s\n",
      "  training loss (in-iteration): \t0.005277\n",
      "Epoch 15 of 100 took 42.896s\n",
      "  training loss (in-iteration): \t0.005212\n",
      "Epoch 16 of 100 took 42.933s\n",
      "  training loss (in-iteration): \t0.005149\n",
      "Epoch 17 of 100 took 42.865s\n",
      "  training loss (in-iteration): \t0.005087\n",
      "Epoch 18 of 100 took 43.371s\n",
      "  training loss (in-iteration): \t0.005028\n",
      "Epoch 19 of 100 took 44.307s\n",
      "  training loss (in-iteration): \t0.004973\n",
      "Epoch 20 of 100 took 42.572s\n",
      "  training loss (in-iteration): \t0.004920\n",
      "Epoch 21 of 100 took 43.011s\n",
      "  training loss (in-iteration): \t0.004866\n",
      "Epoch 22 of 100 took 45.721s\n",
      "  training loss (in-iteration): \t0.004808\n",
      "Epoch 23 of 100 took 43.033s\n",
      "  training loss (in-iteration): \t0.004754\n",
      "Epoch 24 of 100 took 43.138s\n",
      "  training loss (in-iteration): \t0.004703\n",
      "Epoch 25 of 100 took 42.518s\n",
      "  training loss (in-iteration): \t0.004655\n",
      "Epoch 26 of 100 took 43.789s\n",
      "  training loss (in-iteration): \t0.004607\n",
      "Epoch 27 of 100 took 43.993s\n",
      "  training loss (in-iteration): \t0.004562\n",
      "Epoch 28 of 100 took 47.716s\n",
      "  training loss (in-iteration): \t0.004524\n",
      "Epoch 29 of 100 took 46.185s\n",
      "  training loss (in-iteration): \t0.004489\n",
      "Epoch 30 of 100 took 44.316s\n",
      "  training loss (in-iteration): \t0.004450\n",
      "Epoch 31 of 100 took 44.572s\n",
      "  training loss (in-iteration): \t0.004414\n",
      "Epoch 32 of 100 took 42.629s\n",
      "  training loss (in-iteration): \t0.004370\n",
      "Epoch 33 of 100 took 51.214s\n",
      "  training loss (in-iteration): \t0.004327\n",
      "Epoch 34 of 100 took 57.318s\n",
      "  training loss (in-iteration): \t0.004294\n",
      "Epoch 35 of 100 took 51.314s\n",
      "  training loss (in-iteration): \t0.004265\n",
      "Epoch 36 of 100 took 43.923s\n",
      "  training loss (in-iteration): \t0.004243\n",
      "Epoch 37 of 100 took 44.048s\n",
      "  training loss (in-iteration): \t0.004223\n",
      "Epoch 38 of 100 took 43.630s\n",
      "  training loss (in-iteration): \t0.004197\n",
      "Epoch 39 of 100 took 43.580s\n",
      "  training loss (in-iteration): \t0.004174\n",
      "Epoch 40 of 100 took 43.276s\n",
      "  training loss (in-iteration): \t0.004156\n",
      "Epoch 41 of 100 took 43.561s\n",
      "  training loss (in-iteration): \t0.004144\n",
      "Epoch 42 of 100 took 45.473s\n",
      "  training loss (in-iteration): \t0.004132\n",
      "Epoch 43 of 100 took 44.775s\n",
      "  training loss (in-iteration): \t0.004115\n",
      "Epoch 44 of 100 took 43.469s\n",
      "  training loss (in-iteration): \t0.004093\n",
      "Epoch 45 of 100 took 43.168s\n",
      "  training loss (in-iteration): \t0.004064\n",
      "Epoch 46 of 100 took 44.600s\n",
      "  training loss (in-iteration): \t0.004025\n",
      "Epoch 47 of 100 took 42.915s\n",
      "  training loss (in-iteration): \t0.003987\n",
      "Epoch 48 of 100 took 42.640s\n",
      "  training loss (in-iteration): \t0.003950\n",
      "Epoch 49 of 100 took 44.439s\n",
      "  training loss (in-iteration): \t0.003914\n",
      "Epoch 50 of 100 took 44.495s\n",
      "  training loss (in-iteration): \t0.003889\n",
      "Epoch 51 of 100 took 43.858s\n",
      "  training loss (in-iteration): \t0.003873\n",
      "Epoch 52 of 100 took 43.562s\n",
      "  training loss (in-iteration): \t0.003855\n",
      "Epoch 53 of 100 took 43.027s\n",
      "  training loss (in-iteration): \t0.003831\n",
      "Epoch 54 of 100 took 42.995s\n",
      "  training loss (in-iteration): \t0.003801\n",
      "Epoch 55 of 100 took 44.607s\n",
      "  training loss (in-iteration): \t0.003771\n",
      "Epoch 56 of 100 took 45.251s\n",
      "  training loss (in-iteration): \t0.003747\n",
      "Epoch 57 of 100 took 49.655s\n",
      "  training loss (in-iteration): \t0.003735\n",
      "Epoch 58 of 100 took 52.735s\n",
      "  training loss (in-iteration): \t0.003726\n",
      "Epoch 59 of 100 took 49.479s\n",
      "  training loss (in-iteration): \t0.003712\n"
     ]
    }
   ],
   "source": [
    "train_network(net4_group3, criterion, optimizer, mask_input, mask_output,\n",
    "              shape_input=(batch_size, 3, int(dim_x / 2), int(dim_x)),\n",
    "              shape_output=(batch_size, 3, int(dim_x / 2), int(dim_x / 2)),\n",
    "              num_epochs=100\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 1 took 44.164s\n",
      "  training loss (in-iteration): \t0.003142\n"
     ]
    }
   ],
   "source": [
    "train_network(net4_group3, criterion, optimizer, mask_input, mask_output,\n",
    "              shape_input=(batch_size, 3, int(dim_x / 2), int(dim_x)),\n",
    "              shape_output=(batch_size, 3, int(dim_x / 2), int(dim_x / 2)),\n",
    "              num_epochs=1\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koloskov/anaconda3/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type Net4Group3. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(net4_group3, \"net4.group3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAABjCAYAAACvxMuwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAABNpJREFUeJzt3M+L3PUdx/HXNzNZtSELmx21K5vs\nllI8iNpDCVGkpX+C4FlyaI+CB48ivYr02P9B2lvxFOLFiwRtbNMEgookPWh+kdagu8n+mG9PWyx4\n2C3vdN+0j8d5eX1nZz/z3efMwg7jOAYAoIMjh/0AAAD2CBMAoA1hAgC0IUwAgDaECQDQhjABANoQ\nJgBAG8IEAGhDmAAAbUwP8sWz2WxcX18vufDuzk7Jzp75fF62tf1gs2wrSTa+vlu29chjx8q2kuTY\n0nLZ1pEjk7Kta9eu5c6dO0PZ4AEsLy+Pq6urJVuTSd1zkiRj4Tm/e+tG2VaSDJOjZVtLy7OyrSRJ\n4UmqvNckyaVLl+6M4/h46eg+zWazcW1trWSr+nmp/K/k8+0HZVtJMu5ulW0NR39QtpUkRwrvOdX3\nr4sXL+7rrB8oTNbX13PhwoX//FF9x72v/16ys+f+xjdlW19+fqVsK0n+/N67ZVs/eu5nZVtJcubl\ns2Vbjx47XrZ1+vTpsq2DWl1dzblz50q2FhcXS3b23P+m7pz/4XfvlG0lyWTxibKtV87+umwrScbC\nMLlf/MZlZWXleungAaytrZXd0zc2Nkp29uxsbZdt3bv5RdlWkuzc/VvZ1qOrz5dtJcnCsbp7ztLS\nUtlWkiwsLOzrrPtTDgDQhjABANoQJgBAG8IEAGhDmAAAbQgTAKANYQIAtCFMAIA2hAkA0IYwAQDa\nECYAQBvCBABoQ5gAAG0IEwCgDWECALQhTACANoQJANDG9LAuvLnxbenetU8vl23d+uT9sq0keXJl\nVra1O+6UbSVJhto5Hq5hOinb2pwslG0lyV8+/lPZ1olTPynbSpIXXjxTN+bt3PeaTo+W7o3jWLa1\n8+0/yraSZLK7Vba1MK37PpP6n8Nh8BIDANoQJgBAG8IEAGhDmAAAbQgTAKANYQIAtCFMAIA2hAkA\n0IYwAQDaECYAQBvCBABoQ5gAAG0IEwCgDWECALQhTACANoQJANCGMAEA2hAmAEAbwgQAaGN6WBe+\nv7lRunf71s2yras3t8q2kuTGsFq29cvnXirbSpJhslC6x8N19epnZVu//+P5sq0k+ermjbKtjz65\nXLaVJG/95q2yrZd+Xvsa5PvtFv6OWHhQNpUkmUwXy7aGTMq2kmQYSucOhU9MAIA2hAkA0IYwAQDa\nECYAQBvCBABoQ5gAAG0IEwCgDWECALQhTACANoQJANCGMAEA2hAmAEAbwgQAaEOYAABtCBMAoA1h\nAgC0IUwAgDaECQDQxvSwLry9vVW6tzPWNdbxE7OyrSR5/plny7ZOnnqybCtJxnEs3ePfzefz0r0r\nf71StrV08umyrSQ5vrJWtvXmG6+XbSXJylM/LNsahqFs63/JfL5buvfhB+fLtt5+57dlW0ny2q9e\nLdv6xY9/WraV1N9zDoNPTACANoQJANCGMAEA2hAmAEAbwgQAaEOYAABtCBMAoA1hAgC0IUwAgDaE\nCQDQhjABANoQJgBAG8IEAGhDmAAAbQgTAKANYQIAtCFMAIA2hAkA0IYwAQDaGMZx3P8XD8PtJNcf\n3sOBf1kbx/Hxw7iwc85/mbPO/4t9nfUDhQkAwMPkTzkAQBvCBABoQ5gAAG0IEwCgDWECALQhTACA\nNoQJANCGMAEA2hAmAEAb/wTwIsxwt5X92wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f273e2f8748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_batch_input = X_batch[:, 0]\n",
    "X_batch_input = X_batch_input[mask_input.nonzero()].view((batch_size, 3, int(dim_x / 2), int(dim_x)))\n",
    "X_batch_input = Variable(X_batch_input).cuda()\n",
    "plot_gallery(X_batch_input.cpu().data, 8, 8, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAACxCAYAAADwMnaUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f273e2f8908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_gallery(net4_group3(X_batch_input,\n",
    "                 keypoints=autoencoder_stickmans(Variable(y_batch[:, 1]).cuda())[1],\n",
    "                 embeddings=autoencoder_people(Variable(X_batch[:, 0]).cuda())[1]).cpu().data,\n",
    "             8, 8, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAACxCAYAAADwMnaUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAABTJJREFUeJzt3b9r3HUcx/Hv9a6D1A4pl4RAucNJ\n9B9wKv74Nxy6OBQEdXZTEOrg6uLm7uBQ3FwSkGwNOIgBaWobhERTh5ofmnyddMon5kyu75f4eMx3\nfF7HfS/3vLshg77vOwCABFeqBwAA/EWYAAAxhAkAEEOYAAAxhAkAEEOYAAAxhAkAEEOYAAAxhAkA\nEEOYAAAxRrPceDwe95PJZF5bLuTo8KB6QtPhk93qCU3Xl29WTzjVw4db3e7uz4OKs8fjcT+dTiuO\n/kcnJ7n/QmLnxx+qJzQt3nyhekLT/Y37u33fL1acnfw3/fj3w+oJTcdHue83o+euV09o2tjYONe1\nPlOYTCaTbnV19d+vmqNHD76vntC0+eVn1ROaXn/34+oJp7p169Wys6fTabe+vl52/lme/rZfPaHp\n03ferJ7QdOeTz6snNN0YL2xVnT2ZTLq1tbWq48+0t71ZPaHp6ePvqic0Lbz8RvWEpqWlpXNd637K\nAQBiCBMAIIYwAQBiCBMAIIYwAQBiCBMAIIYwAQBiCBMAIIYwAQBiCBMAIIYwAQBiCBMAIIYwAQBi\nCBMAIIYwAQBiCBMAIIYwAQBiCBMAIIYwAQBiCBMAIIYwAQBiCBMAIIYwAQBiCBMAIIYwAQBiCBMA\nIIYwAQBiCBMAIIYwAQBiCBMAIIYwAQBiCBMAIIYwAQBiCBMAIIYwAQBiCBMAIMaoesBl+enBZvWE\npq/3VqonNL32x1H1hNP1J9ULIt396G71hKYvvsl9Dd7eP6iewIz2dx5XT2hauLZcPaFpOBxWT7gw\n35gAADGECQAQQ5gAADGECQAQQ5gAADGECQAQQ5gAADGECQAQQ5gAADGECQAQQ5gAADGECQAQQ5gA\nADGECQAQQ5gAADGECQAQQ5gAADGECQAQQ5gAADGECQAQQ5gAADGECQAQQ5gAADGECQAQQ5gAADGE\nCQAQQ5gAADGECQAQQ5gAADGECQAQQ5gAADGECQAQQ5gAADGECQAQQ5gAADGECQAQQ5gAADGECQAQ\nYzT7XfrLX3EJth9tV09o2ru6XD3hDMPqAQ2D6gGRRsPU56vrjvYPqic0nZwcV09gRs+vvFg9oenJ\n5rfVE5pu9Jnv0bPwjQkAEEOYAAAxhAkAEEOYAAAxhAkAEEOYAAAxhAkAEEOYAAAxhAkAEEOYAAAx\nhAkAEEOYAAAxhAkAEEOYAAAxhAkAEEOYAAAxhAkAEEOYAAAxhAkAEEOYAAAxhAkAEEOYAAAxhAkA\nEEOYAAAxhAkAEEOYAAAxhAkAEEOYAAAxhAkAEEOYAAAxhAkAEEOYAAAxhAkAEEOYAAAxhAkAEEOY\nAAAxhAkAEEOYAAAxRrPeoe/nMePixtdmfijPzAe3X6me0BT6dNLw3ttvVU9ourryUvWEpsEVn8H+\na7Z/+bV6QtOd9z+sntB0795X1RMuzKsVAIghTACAGMIEAIghTACAGMIEAIghTACAGMIEAIghTACA\nGMIEAIghTACAGMIEAIghTACAGMIEAIghTACAGMIEAIghTACAGMIEAIghTACAGMIEAIghTACAGMIE\nAIghTACAGMIEAIghTACAGMIEAIghTACAGMIEAIghTACAGMIEAIghTACAGMIEAIghTACAGMIEAIgh\nTACAGMIEAIghTACAGMIEAIghTACAGIO+789/48Fgp+u6rfnNgb9N+75frDjYdc4z5lrn/+Jc1/pM\nYQIAME9+ygEAYggTACCGMAEAYggTACCGMAEAYggTACCGMAEAYggTACCGMAEAYvwJf76MjbtFt6QA\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f273e293f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_batch_output = X_batch[:, 1]\n",
    "X_batch_output = X_batch_output[mask_output.nonzero()].view((batch_size, 3, int(dim_x / 2), int(dim_x / 2)))\n",
    "plot_gallery(X_batch_output, 8, 8, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_group4_network(net, criterion, optimizer, mask_input, mask_output, shape_output, num_epochs):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        net.train(True)\n",
    "        i = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            i += 1\n",
    "            if X_batch.shape[0] != batch_size:\n",
    "                continue\n",
    "            X_batch_input = X_batch[:, 0]\n",
    "            X_batch_input[(~mask_input).nonzero()] = 0\n",
    "            X_batch_input = Variable(X_batch_input).cuda()\n",
    "\n",
    "            X_batch_output = X_batch[:, 1]\n",
    "            X_batch_output = X_batch_output[mask_output.nonzero()].view(shape_output)\n",
    "            X_batch_output = Variable(X_batch_output).cuda()\n",
    "\n",
    "            output_img = net(X_batch_input,\n",
    "                                     keypoints=autoencoder_stickmans(Variable(y_batch[:, 1]).cuda())[1],\n",
    "                                     embeddings=autoencoder_people(Variable(X_batch[:, 0]).cuda())[1])\n",
    "            loss = criterion(output_img, X_batch_output)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.append(loss.cpu().data.numpy()[0])\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "    #     autoencoder.train(False) # disable dropout / use averages for batch_norm\n",
    "    #     for X_batch, y_batch in val_loader:\n",
    "    #         X_batch_0 = Variable(y_batch[:, 0]).cuda()\n",
    "    #         output_img, _ = autoencoder(X_batch_0)\n",
    "    #         val_loss.append(criterion(output_img, X_batch_0).cpu().data.numpy()[0])\n",
    "    #         X_batch_1 = Variable(y_batch[:, 1]).cuda()\n",
    "    #         output_img, _ = autoencoder(X_batch_1)\n",
    "    #         val_loss.append(criterion(output_img, X_batch_1).cpu().data.numpy()[0])\n",
    "\n",
    "        print \n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "            np.mean(train_loss[-2 * len(train_loader):])))\n",
    "    #     print(\"  validation loss: \\t\\t\\t{:.6f}\".format(\n",
    "    #         np.mean(val_loss[-2 * len(val_loader):])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net4Group4(nn.Module):\n",
    "    \"\"\"\n",
    "        Network for predictions group 2 images based on group 1.\n",
    "        input size: 4x4\n",
    "        output size: 4x4\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers=[1], bottelneck_size=32):\n",
    "        super(Net4Group4, self).__init__()\n",
    "        \n",
    "        self.first_part = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=2, stride=1, padding=1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.up_1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 64, 1, stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv_1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=(2, 2), stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=(2, 2), stride=1, padding=0),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 3, kernel_size=(2, 2), stride=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, keypoints, embeddings):\n",
    "        x = self.first_part(x)\n",
    "        x = torch.cat((x, keypoints, embeddings), dim=1)\n",
    "        x = self.up_1(x)\n",
    "        x = self.conv_1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask_input_3 = np.zeros((batch_size, 3, dim_x, dim_x)).astype(bool)\n",
    "for m in range(batch_size):\n",
    "    for n in range(3):\n",
    "        for i in range(dim_x):\n",
    "            for j in range(dim_x):\n",
    "                if i % 2 == 0 or j % 2 == 0:\n",
    "                    mask_input_3[m][n][i][j] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask_output_3 = np.zeros((batch_size, 3, dim_x, dim_x)).astype(bool)\n",
    "for m in range(batch_size):\n",
    "    for n in range(3):\n",
    "        for i in range(dim_x):\n",
    "            for j in range(dim_x):\n",
    "                if i % 2 == 1 and j % 2 == 1:\n",
    "                    mask_output_3[m][n][i][j] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "net4_group4 = Net4Group4().cuda()\n",
    "optimizer = optim.Adam(net4_group4.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 42.841s\n",
      "  training loss (in-iteration): \t0.015678\n",
      "Epoch 2 of 100 took 43.864s\n",
      "  training loss (in-iteration): \t0.011817\n",
      "Epoch 3 of 100 took 45.041s\n",
      "  training loss (in-iteration): \t0.007518\n",
      "Epoch 4 of 100 took 44.261s\n",
      "  training loss (in-iteration): \t0.006845\n",
      "Epoch 5 of 100 took 42.551s\n",
      "  training loss (in-iteration): \t0.006491\n",
      "Epoch 6 of 100 took 42.373s\n",
      "  training loss (in-iteration): \t0.006247\n",
      "Epoch 7 of 100 took 43.562s\n",
      "  training loss (in-iteration): \t0.006060\n",
      "Epoch 8 of 100 took 43.679s\n",
      "  training loss (in-iteration): \t0.005906\n",
      "Epoch 9 of 100 took 43.410s\n",
      "  training loss (in-iteration): \t0.005773\n",
      "Epoch 10 of 100 took 44.620s\n",
      "  training loss (in-iteration): \t0.005650\n",
      "Epoch 11 of 100 took 42.912s\n",
      "  training loss (in-iteration): \t0.005543\n",
      "Epoch 12 of 100 took 44.131s\n",
      "  training loss (in-iteration): \t0.005446\n",
      "Epoch 13 of 100 took 43.266s\n",
      "  training loss (in-iteration): \t0.005356\n",
      "Epoch 14 of 100 took 44.248s\n",
      "  training loss (in-iteration): \t0.005278\n",
      "Epoch 15 of 100 took 43.180s\n",
      "  training loss (in-iteration): \t0.005206\n",
      "Epoch 16 of 100 took 42.912s\n",
      "  training loss (in-iteration): \t0.005137\n",
      "Epoch 17 of 100 took 42.893s\n",
      "  training loss (in-iteration): \t0.005075\n",
      "Epoch 18 of 100 took 42.968s\n",
      "  training loss (in-iteration): \t0.005011\n",
      "Epoch 19 of 100 took 45.824s\n",
      "  training loss (in-iteration): \t0.004944\n",
      "Epoch 20 of 100 took 51.136s\n",
      "  training loss (in-iteration): \t0.004887\n",
      "Epoch 21 of 100 took 55.708s\n",
      "  training loss (in-iteration): \t0.004837\n",
      "Epoch 22 of 100 took 46.289s\n",
      "  training loss (in-iteration): \t0.004785\n",
      "Epoch 23 of 100 took 43.964s\n",
      "  training loss (in-iteration): \t0.004730\n",
      "Epoch 24 of 100 took 43.620s\n",
      "  training loss (in-iteration): \t0.004683\n",
      "Epoch 25 of 100 took 42.654s\n",
      "  training loss (in-iteration): \t0.004632\n",
      "Epoch 26 of 100 took 44.597s\n",
      "  training loss (in-iteration): \t0.004583\n",
      "Epoch 27 of 100 took 44.806s\n",
      "  training loss (in-iteration): \t0.004542\n",
      "Epoch 28 of 100 took 44.058s\n",
      "  training loss (in-iteration): \t0.004494\n",
      "Epoch 29 of 100 took 43.098s\n",
      "  training loss (in-iteration): \t0.004450\n",
      "Epoch 30 of 100 took 43.803s\n",
      "  training loss (in-iteration): \t0.004419\n",
      "Epoch 31 of 100 took 42.686s\n",
      "  training loss (in-iteration): \t0.004385\n",
      "Epoch 32 of 100 took 42.463s\n",
      "  training loss (in-iteration): \t0.004350\n",
      "Epoch 33 of 100 took 42.893s\n",
      "  training loss (in-iteration): \t0.004324\n",
      "Epoch 34 of 100 took 43.998s\n",
      "  training loss (in-iteration): \t0.004298\n",
      "Epoch 35 of 100 took 43.144s\n",
      "  training loss (in-iteration): \t0.004270\n",
      "Epoch 36 of 100 took 45.284s\n",
      "  training loss (in-iteration): \t0.004243\n",
      "Epoch 37 of 100 took 42.905s\n",
      "  training loss (in-iteration): \t0.004209\n",
      "Epoch 38 of 100 took 44.124s\n",
      "  training loss (in-iteration): \t0.004165\n",
      "Epoch 39 of 100 took 43.038s\n",
      "  training loss (in-iteration): \t0.004117\n",
      "Epoch 40 of 100 took 42.733s\n",
      "  training loss (in-iteration): \t0.004079\n",
      "Epoch 41 of 100 took 42.588s\n",
      "  training loss (in-iteration): \t0.004045\n",
      "Epoch 42 of 100 took 42.833s\n",
      "  training loss (in-iteration): \t0.004012\n",
      "Epoch 43 of 100 took 43.497s\n",
      "  training loss (in-iteration): \t0.003984\n",
      "Epoch 44 of 100 took 43.896s\n",
      "  training loss (in-iteration): \t0.003965\n",
      "Epoch 45 of 100 took 43.452s\n",
      "  training loss (in-iteration): \t0.003953\n",
      "Epoch 46 of 100 took 43.136s\n",
      "  training loss (in-iteration): \t0.003936\n",
      "Epoch 47 of 100 took 43.461s\n",
      "  training loss (in-iteration): \t0.003916\n",
      "Epoch 48 of 100 took 43.820s\n",
      "  training loss (in-iteration): \t0.003889\n",
      "Epoch 49 of 100 took 42.434s\n",
      "  training loss (in-iteration): \t0.003858\n",
      "Epoch 50 of 100 took 43.336s\n",
      "  training loss (in-iteration): \t0.003834\n",
      "Epoch 51 of 100 took 43.616s\n",
      "  training loss (in-iteration): \t0.003815\n",
      "Epoch 52 of 100 took 44.188s\n",
      "  training loss (in-iteration): \t0.003790\n",
      "Epoch 53 of 100 took 44.119s\n",
      "  training loss (in-iteration): \t0.003755\n",
      "Epoch 54 of 100 took 46.123s\n",
      "  training loss (in-iteration): \t0.003726\n",
      "Epoch 55 of 100 took 45.551s\n",
      "  training loss (in-iteration): \t0.003700\n",
      "Epoch 56 of 100 took 44.151s\n",
      "  training loss (in-iteration): \t0.003673\n",
      "Epoch 57 of 100 took 45.046s\n",
      "  training loss (in-iteration): \t0.003654\n",
      "Epoch 58 of 100 took 42.888s\n",
      "  training loss (in-iteration): \t0.003646\n",
      "Epoch 59 of 100 took 42.647s\n",
      "  training loss (in-iteration): \t0.003643\n",
      "Epoch 60 of 100 took 47.043s\n",
      "  training loss (in-iteration): \t0.003641\n",
      "Epoch 61 of 100 took 50.082s\n",
      "  training loss (in-iteration): \t0.003633\n",
      "Epoch 62 of 100 took 50.576s\n",
      "  training loss (in-iteration): \t0.003617\n",
      "Epoch 63 of 100 took 50.718s\n",
      "  training loss (in-iteration): \t0.003602\n",
      "Epoch 64 of 100 took 42.391s\n",
      "  training loss (in-iteration): \t0.003589\n",
      "Epoch 65 of 100 took 42.328s\n",
      "  training loss (in-iteration): \t0.003577\n",
      "Epoch 66 of 100 took 43.055s\n",
      "  training loss (in-iteration): \t0.003556\n",
      "Epoch 67 of 100 took 43.482s\n",
      "  training loss (in-iteration): \t0.003527\n",
      "Epoch 68 of 100 took 44.209s\n",
      "  training loss (in-iteration): \t0.003504\n",
      "Epoch 69 of 100 took 43.683s\n",
      "  training loss (in-iteration): \t0.003492\n",
      "Epoch 70 of 100 took 42.805s\n",
      "  training loss (in-iteration): \t0.003481\n",
      "Epoch 71 of 100 took 42.461s\n",
      "  training loss (in-iteration): \t0.003473\n",
      "Epoch 72 of 100 took 43.174s\n",
      "  training loss (in-iteration): \t0.003460\n",
      "Epoch 73 of 100 took 43.346s\n",
      "  training loss (in-iteration): \t0.003438\n",
      "Epoch 74 of 100 took 43.662s\n",
      "  training loss (in-iteration): \t0.003419\n",
      "Epoch 75 of 100 took 44.330s\n",
      "  training loss (in-iteration): \t0.003395\n",
      "Epoch 76 of 100 took 43.143s\n",
      "  training loss (in-iteration): \t0.003373\n",
      "Epoch 77 of 100 took 44.771s\n",
      "  training loss (in-iteration): \t0.003362\n",
      "Epoch 78 of 100 took 43.754s\n",
      "  training loss (in-iteration): \t0.003348\n",
      "Epoch 79 of 100 took 43.705s\n",
      "  training loss (in-iteration): \t0.003330\n",
      "Epoch 80 of 100 took 43.266s\n",
      "  training loss (in-iteration): \t0.003318\n",
      "Epoch 81 of 100 took 42.520s\n",
      "  training loss (in-iteration): \t0.003312\n",
      "Epoch 82 of 100 took 42.460s\n",
      "  training loss (in-iteration): \t0.003306\n",
      "Epoch 83 of 100 took 44.132s\n",
      "  training loss (in-iteration): \t0.003290\n",
      "Epoch 84 of 100 took 43.738s\n",
      "  training loss (in-iteration): \t0.003281\n",
      "Epoch 85 of 100 took 44.324s\n",
      "  training loss (in-iteration): \t0.003287\n",
      "Epoch 86 of 100 took 43.639s\n",
      "  training loss (in-iteration): \t0.003297\n",
      "Epoch 87 of 100 took 43.444s\n",
      "  training loss (in-iteration): \t0.003303\n",
      "Epoch 88 of 100 took 43.656s\n",
      "  training loss (in-iteration): \t0.003291\n",
      "Epoch 89 of 100 took 44.906s\n",
      "  training loss (in-iteration): \t0.003268\n",
      "Epoch 90 of 100 took 43.171s\n",
      "  training loss (in-iteration): \t0.003253\n",
      "Epoch 91 of 100 took 42.560s\n",
      "  training loss (in-iteration): \t0.003237\n",
      "Epoch 92 of 100 took 45.077s\n",
      "  training loss (in-iteration): \t0.003216\n",
      "Epoch 93 of 100 took 49.374s\n",
      "  training loss (in-iteration): \t0.003198\n",
      "Epoch 94 of 100 took 50.932s\n",
      "  training loss (in-iteration): \t0.003179\n",
      "Epoch 95 of 100 took 47.142s\n",
      "  training loss (in-iteration): \t0.003159\n",
      "Epoch 96 of 100 took 46.255s\n",
      "  training loss (in-iteration): \t0.003144\n",
      "Epoch 97 of 100 took 47.251s\n",
      "  training loss (in-iteration): \t0.003124\n",
      "Epoch 98 of 100 took 44.233s\n",
      "  training loss (in-iteration): \t0.003101\n",
      "Epoch 99 of 100 took 43.480s\n",
      "  training loss (in-iteration): \t0.003091\n",
      "Epoch 100 of 100 took 42.218s\n",
      "  training loss (in-iteration): \t0.003090\n"
     ]
    }
   ],
   "source": [
    "train_group4_network(net4_group4, criterion, optimizer, mask_input_3, mask_output_3,\n",
    "              shape_output=(batch_size, 3, int(dim_x / 2), int(dim_x / 2)),\n",
    "              num_epochs=100\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koloskov/anaconda3/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type Net4Group4. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(net4_group4, \"net4.group4.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAACxCAYAAADwMnaUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n41+YkHFpcQjEuwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2748111f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_batch_input = X_batch[:, 0].clone()\n",
    "X_batch_input[(~mask_input_3).nonzero()] = 0\n",
    "X_batch_input = Variable(X_batch_input).cuda()\n",
    "plot_gallery(X_batch_input.cpu().data, 8, 8, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask_group_1 = np.zeros((batch_size, 3, dim_x, dim_x)).astype(bool)\n",
    "for m in range(batch_size):\n",
    "    for n in range(3):\n",
    "        for i in range(dim_x):\n",
    "            for j in range(dim_x):\n",
    "                if i % 2 == 0 and j % 2 == 0:\n",
    "                    mask_group_1[m][n][i][j] = True\n",
    "\n",
    "mask_group_2 = np.zeros((batch_size, 3, dim_x, dim_x)).astype(bool)\n",
    "for m in range(batch_size):\n",
    "    for n in range(3):\n",
    "        for i in range(dim_x):\n",
    "            for j in range(dim_x):\n",
    "                if i % 2 == 0 and j % 2 == 1:\n",
    "                    mask_group_2[m][n][i][j] = True\n",
    "\n",
    "mask_group_3 = np.zeros((batch_size, 3, dim_x, dim_x)).astype(bool)\n",
    "for m in range(batch_size):\n",
    "    for n in range(3):\n",
    "        for i in range(dim_x):\n",
    "            for j in range(dim_x):\n",
    "                if i % 2 == 1 and j % 2 == 0:\n",
    "                    mask_group_3[m][n][i][j] = True\n",
    "\n",
    "mask_group_4 = np.zeros((batch_size, 3, dim_x, dim_x)).astype(bool)\n",
    "for m in range(batch_size):\n",
    "    for n in range(3):\n",
    "        for i in range(dim_x):\n",
    "            for j in range(dim_x):\n",
    "                if i % 2 == 1 and j % 2 == 1:\n",
    "                    mask_group_4[m][n][i][j] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_input_into_image(X_batch):\n",
    "    X_batch_input = X_batch[:, 0].clone()\n",
    "    X_batch_input = X_batch_input[mask_input_1.nonzero()].view((batch_size, 3, int(dim_x / 2), int(dim_x / 2)))\n",
    "    X_batch_input = Variable(X_batch_input).cuda()\n",
    "    \n",
    "    group_1 = X_batch_input\n",
    "    group_2 = net4_group2(X_batch_input,\n",
    "                 keypoints=autoencoder_stickmans(Variable(y_batch[:, 1]).cuda())[1],\n",
    "                 embeddings=autoencoder_people(Variable(X_batch[:, 0]).cuda())[1]).cpu().data\n",
    "    \n",
    "    X_batch_input = X_batch[:, 0].clone()\n",
    "    X_batch_input = X_batch_input[mask_input_2.nonzero()].view((batch_size, 3, int(dim_x / 2), int(dim_x)))\n",
    "    X_batch_input = Variable(X_batch_input).cuda()\n",
    "    \n",
    "    group_3 = net4_group3(X_batch_input,\n",
    "                 keypoints=autoencoder_stickmans(Variable(y_batch[:, 1]).cuda())[1],\n",
    "                 embeddings=autoencoder_people(Variable(X_batch[:, 0]).cuda())[1]).cpu().data\n",
    "    \n",
    "    X_batch_input = X_batch[:, 0].clone()\n",
    "    X_batch_input[(~mask_input_3).nonzero()] = 0\n",
    "    X_batch_input = Variable(X_batch_input).cuda()\n",
    "    \n",
    "    group_4 = net4_group4(X_batch_input,\n",
    "                 keypoints=autoencoder_stickmans(Variable(y_batch[:, 1]).cuda())[1],\n",
    "                 embeddings=autoencoder_people(Variable(X_batch[:, 0]).cuda())[1]).cpu().data\n",
    "    \n",
    "    final_image = torch.zeros_like(X_batch[:, 1]).double()\n",
    "    final_image[mask_group_1.nonzero()] = group_1.view(-1).cpu().data.double()\n",
    "    final_image[mask_group_2.nonzero()] = group_2.view(-1).double()\n",
    "    final_image[mask_group_3.nonzero()] = group_3.view(-1).double()\n",
    "    final_image[mask_group_4.nonzero()] = group_4.view(-1).double()\n",
    "    \n",
    "    plot_gallery(X_batch[:, 1], 8, 8, 1, 3)\n",
    "    plot_gallery(final_image, 8, 8, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAACxCAYAAADwMnaUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAB89JREFUeJzt3T9vXQcdxvHn3HvtOIlTktZpSqva\nQ1RVDI0EARUJtQPMsPMGKhZGeBPsSOxsDFVfABKQoipL0gGlqoJUEok2zZ8mzj/HN/Y9DDQIVCHi\n6pf61/D5zPZzrnNOzv3e48HDOI4BAOhgst8vAADgEWECALQhTACANoQJANCGMAEA2hAmAEAbwgQA\naEOYAABtCBMAoA1hAgC0MdvLF6+trY3r6+slB14sFiU7j4yFe1v3bpdtJcnO1r2yrcNHny3bSpKl\nlcOle1UuX76c69evD/tx7LW1tXFjY6Nkaxhqf4TKvyBx/27tdX7n5o2yraPHXyjbSpLlAytlW8Wn\nNOfOnbs+juPx2tXHU3lPr/7zJovdnbKt3Qd19+AkmSwtl20NswNlW9Wm02np3vnz5x/rWt9TmKyv\nr+fMmTNf/lX9m+0HWyU7jzwofPO/8N7vy7aS5OpfzpZtvf6Tn5ZtJckLr56uGxvqHsC98cYbZVt7\ntbGxkbNna85ZdZhU9vz7f6q9zv/w9m/Ltn781i/KtpJk4+QrZVtLB/Z02/zfe0tLl0oH92B9fT3v\nvvtuydZ8Pi/ZeWTr9s2yrZsf/rlsK0kOn6j54JIky8+dLNtKksmsLiZWV1fLtj7fe6xr3a9yAIA2\nhAkA0IYwAQDaECYAQBvCBABoQ5gAAG0IEwCgDWECALQhTACANoQJANCGMAEA2hAmAEAbwgQAaEOY\nAABtCBMAoA1hAgC0MduvA4/jonTvzuZnZVu3Ll8o20qSZw5My7Y2b1wp20qSF4bSOZ6w+1v3y7au\nXv5r2VaS7Ny9VbZ19conZVtJcuKll8u2lpZXy7aeJpOh9maymN8u27p/9VLZVpLMlpfKtlaOnyzb\nSpLJpO55w1B8Th+XJyYAQBvCBABoQ5gAAG0IEwCgDWECALQhTACANoQJANCGMAEA2hAmAEAbwgQA\naEOYAABtCBMAoA1hAgC0IUwAgDaECQDQhjABANoQJgBAG8IEAGhDmAAAbcz268DDMJTuPbx3q2zr\n6Orhsq0kmR4+WLZ17dLFsq0kefX7O3Vj0+W6rafEOI6le2+//U7Z1pk/ni3bSpI7n94r29q4crVs\nK0leebBdtrX6TO394Wmxu3W3dO/h5qdlW8+9dLJsK0lWj50o2xrmm2VbSTIsP1+6tx88MQEA2hAm\nAEAbwgQAaEOYAABtCBMAoA1hAgC0IUwAgDaECQDQhjABANoQJgBAG8IEAGhDmAAAbQgTAKANYQIA\ntCFMAIA2hAkA0IYwAQDaECYAQBuz/X4BVW5cu1a29bv3t8q2kuTw6pGyrR9962DZVpKM46Jsayhb\nenqM41i69+GFD8q23jt7rmwrSXYWO2VbP/jsZtlWUn8e+KLpyqHSvZ153fV0cOlo2VaSZF43tbtd\n+36zdOTrfyf2xAQAaEOYAABtCBMAoA1hAgC0IUwAgDaECQDQhjABANoQJgBAG8IEAGhDmAAAbQgT\nAKANYQIAtCFMAIA2hAkA0IYwAQDaECYAQBvCBABoQ5gAAG0IEwCgjdl+HXh3d7d079bm3bKtv92e\nlm0lyfzBctnWd5ZeLtv6p9qflf80ZCjdq/wkMd/aKlxL5g+3y7bGxVi2lSTDpPY88EWTWd19LkmW\nDh0r21ps3izbSpJxd1G2NZns29twW56YAABtCBMAoA1hAgC0IUwAgDaECQDQhjABANoQJgBAG8IE\nAGhDmAAAbQgTAKANYQIAtCFMAIA2hAkA0IYwAQDaECYAQBvCBABoQ5gAAG0IEwCgjdnev2UsOfDD\n+bxk55H7d26Xbb104ljZVpKMK0fLtlYOrZZt8RUYaucm02nd1qRuK0km0y9xO/kvPrp4oWwrSXZ3\ndkr3+KJxrHlveOTAN54v27p15dOyrSR5sP2wbOvQwYNlW0mycqz2POwHT0wAgDaECQDQhjABANoQ\nJgBAG8IEAGhDmAAAbQgTAKANYQIAtCFMAIA2hAkA0IYwAQDaECYAQBvCBABoQ5gAAG0IEwCgDWEC\nALQhTACANoQJANCGMAEA2pjt9RvGsebAO/PtmqHPvXji2bKtX755umwrSTI9UDa1NL9btpUkRacz\nSTIUbj0thqH2X+WHb75etvXBxY/KtpLkXuqu81Onvl22lbg2vwrjuCjd++TajbKtX/36N2VbSbJ5\n507Z1s9/9lbZVpJ898VXSvf2gycmAEAbwgQAaEOYAABtCBMAoA1hAgC0IUwAgDaECQDQhjABANoQ\nJgBAG8IEAGhDmAAAbQgTAKANYQIAtCFMAIA2hAkA0IYwAQDaECYAQBvCBABoY7ZfB97Z3Sndm9+9\nXbY1u3etbCtJlo8cq9uaLsq2ePKGYSjdO/Xaa2Vbr33vUtlWkgwH667zI99cK9tKkkx8BnvSxnGs\n3ZvWvT1dvrFZtpUk8+3tsq2/f/xx2VaSnF58/d8j/G8FANoQJgBAG8IEAGhDmAAAbQgTAKANYQIA\ntCFMAIA2hAkA0IYwAQDaECYAQBvCBABoQ5gAAG0IEwCgDWECALQhTACANoQJANCGMAEA2hAmAEAb\nwgQAaGMYx/Hxv3gYriW59OReDvzLxjiOx/fjwK5zvmKudf5fPNa1vqcwAQB4kvwqBwBoQ5gAAG0I\nEwCgDWECALQhTACANoQJANCGMAEA2hAmAEAbwgQAaOMfzNkz/zBVyNAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f273df81390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAACxCAYAAADwMnaUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNgwTAKANwwQAaMMwAQDa+E/Vl60FbZBZjwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f273e10dcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "convert_input_into_image(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAACxCAYAAADwMnaUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f273df12080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAACxCAYAAADwMnaUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNgwTAKANwwQAaMMwAQDa+E/Vl60FbZBZjwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f273e038e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "convert_input_into_image(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for X_batch, y_batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAACxCAYAAADwMnaUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8SX0Kt+/7AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f273e293390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_gallery(X_batch[:, 0], 8, 8, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAACxCAYAAADwMnaUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f27481113c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_gallery(net4_group4(X_batch_input,\n",
    "                 keypoints=autoencoder_stickmans(Variable(y_batch[:, 1]).cuda())[1],\n",
    "                 embeddings=autoencoder_people(Variable(X_batch[:, 0]).cuda())[1]).cpu().data,\n",
    "             8, 8, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAACxCAYAAADwMnaUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f273e2dd668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_batch_output = X_batch[:, 1]\n",
    "X_batch_output = X_batch_output[mask_output.nonzero()].view((batch_size, 3, int(dim_x / 2), int(dim_x / 2)))\n",
    "plot_gallery(X_batch_output, 8, 8, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
